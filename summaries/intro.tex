\chapter{Introduction}
\label{chapter:intro}
% % % layout
% % % Distributed Computing Challenges
% % % Qualities of DHTs
% % % Hypothesis = These Problems + These qualiteies -> solution
% % % Framework of what these solutions are and what they can do

%TODO: This chapter is a chapter long abstract/introduction.
%TODO: What I want to do, why you should care.
%TODO: Why you should care is applications.  What applications can you use be general but also give wide varity of specific examples
%TODO: Motivational selling point
%TODO: Get some structure!  Move background to background, this section should be purely motivation;  tell what dht's can do right now and what proposed uses there are and what YOU propose.  Also incorperate the challenges you need to overcome.  You can say these things are hard for DHTs and we'll tell you more in background.
%TODO:  Make sure you have roadmap:  Highlevel What you have done, what you plan to do.
%TODO:  A section with list of my papers (publications based on this work and ref the label for where comes up) (seperate list of stuff I've been involved in)




% What do I want to do?
\section{Objective}
Our goal is to create a framework to further generalize Distributed Hash Tables (DHTs) to be used for distributed computing.
Distributed computing platforms need to be scalable, fault-tolerant, and load-balancing.
%The ability to incorporate heterogeneous hardware is a definite benefit.
We will discuss what each of these mean and why they are important in section \ref{sec:challenges}, but briefly:

\begin{itemize}
	\item The system should be able to work effectively no matter how large it gets.
	As the system grows in size, we can expect the overhead to grow in size as well, but at an extremely slower rate.
	\item The more machine integrated into the system, the more we can expect to see hardware failures.
	The system needs to be able to automatically handle these hardware failures.
	\item Having a large number of machines to use is worthless if the amount of work is divided unevenly among the system.
	The same is true if the system hands out larger jobs to less powerful machines or smaller jobs to the more powerful machines.
	%\item We cannot assume that we be able to replace our broken machines with exact replicas, nor do we assume we would want to. 
\end{itemize}


These are many of the same challenges that Peer-to-peer (P2P) file sharing applications have.
Many P2P applications use DHTs to address these challenges, since DHTs are designed with these problems in mind.
We propose that DHTs can be used to create P2P distributed computing platforms that are completely decentralized.
%Rather than keys being assigned to some data, we can assign keys to tasks and automatically distribute those tasks to the responsible nodes
There would be no need for some central organized or scheduler to coordinate the nodes in the network.
Our framework would not be limited to only a P2P context, but could be applied in data centers, a normally centrally organized context.


A successful DHT-based computing platform would need to address the problem of dynamic load-balancing.
This is currently an unsolved problem.
If an application can dynamically reassign work to nodes added at runtime, this opens up new options for resource management.
Similarly. if a distributed computation  is running too slow, new nodes can be added to the network during runtime or idle nodes can boot up more virtual nodes. %(now that I think of it this is two different but highly related problems: internal and external).

%Move this bit to the end?
Chapter \ref{chapter:background} will delve into how DHTs work and examine specific DHTs.
The remainder of the proposal will then discuss the work we plan on doing to demonstrate the viability of using DHTs for distributed computing.



% How I will do it is in the experiments chapter

% Why should you care?
\section{Applications of Distributed Hash Tables}

Distributed Hash Tables have been used in numerous applications:

\begin{itemize}
	\item \textbf{P2P file sharing} is by far the most prominent use of DHTs.  
	The most well-known application is BitTorrent \cite{bittorrent}, which is built on Mainline DHT \cite{mainline}.
	\item DHTs have been used for \textbf{distributed storage} systems \cite{CFS}.
	\item \textbf{Distributed Domain Name Systems} (DNS) have been built upon DHTs \cite{cox2002serving} \cite{pappas2006comparative}.
	Distributed DNSs are much more robust that DNS to orchestrated attacks, but otherwise require more overhead.
	%\item Distributed search (faroo)
	\item Distributed \textbf{machine learning} \cite{liparameter}.
	\item Many \textbf{botnets} are now P2P based and built using well established DHTs \cite{saad2011detecting}. 
	This is because the decentralized nature of P2P systems means there's no single vulnerable location in the botnet.
	\item \textbf{Live video streaming} (BitTorrent live) \cite{mol2009design}.
\end{itemize}

We can see from this list that DHTs are primarily used in P2P applications, but other applications, such as botnets, use DHTs for their decentralization.
We want to use DHTs primarily for their intuitive\footnote{Relatively intuitive, if you know how hash tables work.} way of organizing a distributed system.

We showed  \cite{chordreduce} that a DHT can be to create a distributed computing framework.
We used the same mechanism used in P2P applications that assigns nodes their location in the network to evenly distribute work among members of a DHT.
The most direct application of a DHT distributed computing framework is  a quick and intuitive way to solve embarrassingly parallel problems, such as:
\begin{itemize}
	\item Brute force cryptography.
	\item Genetic algorithms.
	\item Markov chain Monte Carlo methods.
	\item Random forests.
	\item Any problem that could be phrased as a MapReduce problem.
	
\end{itemize}
Unlike the current distributed applications which utilize DHTs, we want to create a complete framework which can be used to build decentralized applications.
We have found no existing projects that provide a means of building your own DHT or DHT based applications. %without a given DHT in mind at least


% So you're diong these things with this tool
\section{Why Use Distributed Hash Tables in Distributed Computing}
% Okay so this is all great, but what's special about a DHT
% First, let's talk about the problems with distributed computing

Using distributed hash tables for distributed computing isn't necessarily the most intuitive step.
To understand why we want to use DHTs for distributed computing, we will first examine some of the more prominent challenges in distributed computing.

\subsection{General Challenges of Distributed Computing}
\label{sec:challenges}

As we mentioned earlier, distributed computing platforms need to be scalable, fault-tolerant, and load-balancing
We will look at these individually:


\begin{description}
	\item[Scalability] - Distributed computing platforms should not be completely static and should grow to accommodate new needs.
	If a platform would be improved by the addition of a new resource, it should be possible to easily add that resource.
	The addition of new workers in a distributed computing framework should be a minimally disruptive process.
	
	This ties into the concept of fault tolerance.
	
	\item[Fault Tolerance]  
	Even in a network that is expected to remain static for long periods of time, the platform still has to deal with node failures.  
	We want our platform to gracefully handle failures during runtime and be able to quickly reassign work to other workers.
	In addition, the network should be equally graceful in handling the introduction of new nodes during runtime.
	A quality of fault-tolerance or \textit{robustness} \footnote{There is apparently a subtle difference between fault-tolerance and robustness, but we will use the two interchangeably here until we get told to stop.}
	
	\item[Load-Balancing]  
	How do you split up a problem then distribute it so that no single worker is under or over-utilized?
	Failing that, how do you minimize the imbalance in work?
	Is there a way to do so at run time?
	A subproblem here is \textit{heterogeneity},\footnote{It could even be considered a problem in its own right.} or how should the system should handle different pieces of hardware with different amounts of computational power.
\end{description}

\subsection{How DHTs Address these Challenges}
%Without getting into the details of what a DHT is, what do they do?

% Rather than avoiding failure by using a centralized manager, we embrace failure 

%We are not geared toward a datacenter-centric setup

\section{Roadmap}

\subsection{Work I have already done}

\subsubsection{Published}

\subsubsection{Unpublished}

\subsection{Summary of Overall Plan}

The specifics are given in Chapter \ref{chapter:experiments}.

\section{Old stuff starts here}



Distributed Computing is well understood to be the approach to take to solve large problems.  
Many problems can be broken up into multiple parts that can be solved simultaneously, yielding a much quicker result than a single worker attacking the problem.
However, there are two broad obstacles in distributed computing.

%Is this 1 problem or two
The first is figuring out the mechanics of efficiently distributing a problem to multiple workers and asynchronously coordinating their effort.  
The second is creating and maintaining the computation platform itself.


Some specific challenges are:


Fortunately, these challenges are not unique to distributed computing.  
Most notably, distributed file storage applications that utilize Distributed Hash Tables are designed to handle these particular challenges.
\section{Distributed Hash Tables}
%\section{DHTs are better for distributed computing under many circumstances}
Distributed Hash Tables (DHTs) are traditionally used as the backbone of structured Peer-to-Peer (P2P) file-sharing applications.
The largest such application by far is Bittorrent \cite{bittorrent}, which is built using Mainline DHT \cite{mainline},  a  derivative of Kademlia \cite{kademlia}.
The number of users on Bittorrent ranges from 15 million to 27 million users daily, with a turnover of 10 million users a day \cite{mainlineMeasure}.

Most research on DHTs assumes that DHTs will be used in the context of a large P2P file-sharing application  (or at least, an application \textit{potentially} incorporating millions of nodes).
This leads the DHT to having particular qualities.
The network must be able to handle members joining and leaving arbitrarily.
The resulting application must be agnostic towards hardware.
The network must be decentralized and split whatever burden there is equally among its members.

In other words, distributed hash tables provide scalability, load-balancing, robustness, and heterogeneity to an application.
Recent applications have leveraged these qualities, since these qualities are desirable in many different frameworks.
For example, one paper \cite{Mateescu2011440} used a DHT as the name resolution layer of a large distributed database.
Research has also been done in using DHTs as an organizing mechanism in distributed machine learning \cite{liparameter}. 

We describe each of the aforementioned qualities and their ramifications below in sections \ref{subsec:ft}, \ref{subsec:lb}, \ref{subsec:scalability}, and \ref{subsec:hetero} .
While these properties are individually enumerated, they are greatly intertwined and the division between their impacts can be somewhat arbitrary.

\subsection{Robustness and Fault-Tolerance}
\label{subsec:ft}
One of the most important assumptions of DHTs is that they are deployed on a non-static network.
DHTs need to be built to account for a high level what is called \textit{churn}.  
Churn refers to the disruption of routing caused by the constant joining and leaving of nodes.
This is mitigated by a few factors.

First, the network is decentralized, with no single node acting as a single point of failure.
This is accomplished by each node in the routing table having a small portion of the both the routing table and the information stored on the DHT (see the Load Balancing property below).

Second is that each DHT has an inexpensive maintenance processes that mitigates the damage caused by churn.
DHTs often integrate a backup process into their protocols so that when a node goes down, one of the neighbors can immediately assume responsibility.
The join process also causes disruption to the network, as affected nodes have adjust their peerlists to accommodating the joiner. 

The last property is that the hash algorithm used to distribute content evenly across the network(again see load balancing) also distributes nodes evenly across the DHT.  
This means that nodes in the same geographic region occupy vastly different positions in the keyspace.  
If an entire geographic region is affected by a network outage, this damage is spread evenly across the DHT, which can be handled.

This property is the most important, as it deals with failure of entire sections of the network, rather than a single node.
Recent research in using DHTs for High End Computing \cite{li2013zht} shows what can happen if we remove this assumption by placing the network that is almost completely static.

The fault tolerance mechanisms in DHTs also provide near constant availability for P2P applications.
The node that is responsible for a particular key can always be found, even when numerous failures or joins occur \cite{chord}.

\subsection{Load Balancing}
\label{subsec:lb}
All Distributed Hash Tables use some kind of consistent hashing algorithm to associate nodes and file identifiers with keys.  
These keys are generated by passing the identifiers into a hash function, typically SHA-160.
The chosen hash function is typically large enough to avoid hash collisions and generates keys in a uniform manner. 
The result of this is that as more nodes join the network, the distribution of nodes in the keyspace becomes more uniform, as does the distribution of files.

However, because this is a random process, it is highly unlikely that each node will be spread evenly throughout the network.
This appears to be a weakness, but can be turned into an advantage in heterogeneous systems by using \textit{virtual nodes} \cite{dynamo} \cite{godfrey2005heterogeneity} .
When a node joins the network, it joins not at one position, but multiple virtual positions in the network \cite{dynamo}.
Using virtual nodes allows load-balance optimization in a heterogeneous network; more powerful machines can create more virtual nodes and handle more of the overall responsibility in the network.

DeCandia et al\. discussed various load balancing techniques that were tested on Dynamo \cite{dynamo}.  
Each node was assigned a certain number of tokens and the node would create a virtual node for each token.
The realization DeCandia et al\. had was that there was no reason to use the same scheme for data partitioning and data placement.
DeCandia et al\. introduced two new strategies which work off assigning nodes equally sized partitions.


% HELP
Under these schemes, each virtual node maps to an ID as before, but the patitions each node is responsible for are equally sized\footnote{Need help here}.

\subsubsection{Heterogeneity}
\label{subsec:hetero}
Heterogenity presents a challenge for load balancing DHTs due to conflicting assumptions and goals. 
DHTs assume that members are usually going to be varied in hardware, while the load-balancing process defined in DHTs treats each node equally.
It is much simpler to treat each node as equal unit.
In other words, DHTs support heterogeneity, but do not attempt to exploit it.

This doesn't mean that heterogeneity cannot be exploited
Nodes can be given addition responsibilities manually, by running multiple instances of the P2P application on the same machine or creating more virtual nodes.
However, this is not a feasible option for any kind of truly decentralized system and would need to be done automatically.
There is no well-known mechanism to that exists to automatically  allocate virtual nodes on the fly \footnote{citation needed, although this can be similar to IRM}. 
A few options present themselves.  % and are discuessed in \ref{}
%add the above if the below is moved

%This might be the wrong place for this
One is to use adapt a request tracking mechanism, such as what is used in IRM, except instead of tracking file requests, it tracks requests that are directed to a particular (real) node. 
If a particular (real) node receives an inordinate amount of requests, the node doing the detecting suggests that the node obtain another token/create another virtual node.
Another strategy is to use the preference lists/successor predecessor lists, and observe the distribution of the workload, adjusting the virtual nodes based on that. 

Dynamic load balancing may not be essential to P2P file-sharing applications, but is absolutely essential to any kind of P2P distributed computation.
In our ChordReduce experiments, we observed that just approximating dynamic load-balancing by simulating high levels of churn noticeably improved results\footnote{We found this by accident, just by testing the network's fault tolerance in regards to a high level of churn}.



%While most DHTs follow this scheme, there is some minor variation on how keys are generated.
%Some DHTs can or do use geographic information to generate keys.
%In VHash, keys are not static, but move according to a simplified spring model.

\subsection{Scalability}
\label{subsec:scalability}
In order to maintain scalability, a DHT has to ensure that as the network grows larger:

\begin{itemize}
    \item Churn does not have a disproportionate overhead.  
    For example, in a 1000 node network, a joining or leaving node will affect only an extremely small subset of these nodes.
    \footnote{We will see that this requirement can  be  relaxed in  very specific cases \cite{li2013zht}.}
    \item Lookup request speeds (usually measured in hops) grow by a much smaller amount, possibly not at all.
\end{itemize}

Using consistent hashing allows the network to scale up incrementally, adding one node at a time \cite{dynamo}.
In addition, each join operation has minimal impact on the network, since a node affects only its immediate neighbors on a join operation.
Similarly, the only nodes that need to react to a node leaving are its neighbors.
This is almost instantaneous if the network is using backups.
Other nodes can be notified of the missing node passively through maintenance or in response to a lookup.

There have been multiple proposed strategies for tackling scalability, and it is these strategies which play the greatest role in driving the variety of DHT architectures. 
Each DHT must strikes a balance between memory cost of the peerlist and lookup time. 
The vast majority of DHTs choose to use $\lg(N)$ sized routing tables and  $\lg(n)$ hops\footnote{log n or log N, matters}. 
Chapter \ref{chapter:background} discusses these tradeoffs in greater detail and how they affect the each DHT.





%\section{Different or subproblem: Certain DHTs are better at one application than another due to differences}
%\subsection{Design Differences Impacts}
%\subsection{Geometries}
%\subsection{Routing Table Construction}
%\subsection{Implementation Differences Impacts}
%\paragraph{Recursive or iterative seek}




%Add these bullets to the above paragraph
%\begin{itemize} 
%	\item DHTs can use consistent hashing supplemented by virtual nodes to efficiently load-balance.
%	The larger the network grows, the more evenly distributed the load becomes. 
%	\item DHTs are highly resilient to damage and can handle abnormally high rates of disruption.  
%	This is extremely desirable in any kind of distributed application %
%	\item Large-scale P2P file sharing applications have been using DHTs for a long time and
%    \item DHTs are extremely good if your problem is embarrassingly par
%    \item Heterogeneity
%\end{itemize}
