\documentclass[10pt,letterpaper]{report}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{venturis2}
\title{Proposal}
\author{Andrew Rosen}
\begin{document}


\maketitle
\setcounter{tocdepth}{4}
\tableofcontents
\newpage
\chapter{Hypotheses}

\section{DHTs are better for distributed computing under many circumstances}
Distributed Hash Tables (DHTs) are traditionally used as the backbone of Peer-to-Peer (P2P) file-sharing applications and research has largely remained in this area.
However, they are seeing increasing use in other applications:

\begin{itemize}
    \item Using a DHT as the name resolution layer of a large distributed database \cite{Mateescu2011440}. %more
    \item Distributed machine learning \cite{liparameter}.
    \item Cloud Provisioning  (P2P cloud Provisioning).
\end{itemize}



\footnote{
Many papers use different terms to describe congruent elements of DHTs, as some terms may make sense 
I shall endeavor to add to confusion by using the following unified terminology:
\begin{enumerate}
    \item[Peerlist] -  The set of all peers that a node knows about.  This is sometimes referred to as the \textit{routing table}, but certain DHTs \cite{tapestry} \cite{pastry} overload the terminology.
    \item[Neighbors] - The subset of peers that are ``closest/adjacent'' to the node in the keyspace, according to the DHT's metric.  In a 1-dimensional ring, such a Chord \cite{chord}, this is the node's \textit{predecessor} and \textit{successor}.
    \item[Fingers] - The subset of the peerlist that  the node is not adjacent to.  These are sometimes referred to as long-hops or shortcuts. 
    \item[Root Node] - The node responsible for a particular key. 
\end{enumerate}
}


\subsection{Really Cool Qualities}
Many DHTs are built upon the assumption that they will be used in some kind of P2P application.
This means a successful application must be deployable on very varied network sizes.
The network must be able to handle members joining and leaving arbitrarily.
The application must be agnostic towards hardware.
The application must be decentralized and split the burden relatively equally among its members.

This leads to DHTs have very specific properties.
While these properties may be individually enumerated, they are greatly intertwined. 
\subsubsection{Robustness and Fault-Tolerance}
One of the most important assumptions of DHTs is that they are deployed on a non-static network.
DHTs need to be built to account for a high level what is called \textit{churn}.  
Churn refers to the disruption of routing caused by the constant joining and leaving of nodes.
This is mitigated by a few factors.

First, the network is decentralized, with no single node acting as a single point of failure.
This is accomplished by each node in the routing table having a small portion of the both the routing table and the information stored on the DHT (see the Load Balancing property below).

Second is that each DHT has an inexpensive maintenance processes that mitigates the damage caused by churn.
DHTs often integrate a backup process into their protocols so that when a node goes down, one of the neighbors can immediately assume responsibility.
The join process also causes disruption to the network, as affected nodes have adjust their peerlists to accommodating the joiner. 

The last property is that the hash algorithm used to distribute content evenly across the network(again see load balancing) also distributes nodes evenly across the DHT.  
This means that nodes in the same geographic region occupy vastly different positions in the keyspace.  
If an entire geographic region is affected by a network outage, this damage is spread evenly across the DHT, which can be handled.

This property is the most important, as it deals with failure of entire sections of the network, rather than a single node.
Recent research in using DHTs for High End Computing \cite{li2013zht} shows what can happen if we remove this assumption by placing the network that is almost completely static.



\subsubsection{Load Balancing}
All Distributed Hash Tables use some kind of consistent hashing algorithm to associate nodes and file identifiers with keys.  
These keys are generated by passing the identifiers into a hash function, typically SHA-160.
The chosen hash function is typically large enough to avoid hash collisions and generates keys in a uniform manner. 
The result of this is that as more nodes join the network, the distribution of nodes in the keyspace becomes more uniform, as does the distribution of files.

However, because this is a random process, it is highly unlikely that each node will be spread evenly throughout the network.
This appears to be a weakness, but can be turned into an advantage in heterogenous systems by using \textit{virtual nodes} \cite{godfrey2005heterogeneity} \cite{dynamo}.
When a node joins the network, it joins not at one position, but multiple virtual positions in the network \cite{dynamo}.
Using virtual nodes allows load-balance optimization in a heterogeneous network; more powerful machines can create more virtual nodes and handle more of  the overall responsibility in the network.
Strategies are discussed further in the Heterogeneity subsection.





%While most DHTs follow this scheme, there is some minor variation on how keys are generated.
%Some DHTs can or do use geographic information to generate keys.
%In VHash, keys are not static, but move according to a simplified spring model.

\subsubsection{Scalability}
In order to maintain scalability, a DHT has to ensure that as the network grows larger:
\begin{itemize}
    \item Churn does not have a disproportionate overhead.
    \item Lookup request speeds (usually measured in hops) grow by a much smaller amount, possibly not at all.
\end{itemize}

Using consistent hashing allows the network to scale up incrementally, adding one node at a time \cite{dynamo}.
In addition, each join operation has minimal impact on the network, since a node affects only its immediate neighbors on a join operation.

Similarly, the only nodes that need to react to a node leaving are immediately.
This is almost instantanius if the network is using backups.
Other nodes can be notified of the missing node passively through maintenance.

There have been multiple proposed strategies for tackling scalability, and it is these strategies which play the greatest role in driving the variety of DHT architectures. 
Each DHT must strikes a balance between memory cost of the peerlist and lookup time. 
The vast majority of DHTs choose a logartihmic sized routing table
Table \ref{tab:tradeoffs} lists various DHTs and methods of balancing cost.


% Perhaps geometries should be included?
% be sure to include join and leave costs.
\begin{table}[h]
	\small
    \centering
    \begin{tabularx}{\textwidth}{ |X|X|X|X|X| }
    \hline
    % Add join leave cost, avgerages and maxs
    DHT & Routing Table Size & Lookup Time & Join/Leave & Comments \\ \hline  
    Chord \cite{chord}, Kademlia \cite{kademlia} & $O(\log n)$ & $O(\log n)$ & & This is where most DHTs fall  \\ \hline
    CAN \cite{can} & $\Omega(2d)$ & $O(n^{\frac{1}{d}})$, average $\frac{d}{4}\cdot n^{\frac{1}{d}}$ & Affects $O(d)$ nodes & $d$ is the number of dimensions \\ \hline
    
    Plaxton-based DHTs, Pastry \cite{pastry}, Tapestry \cite{tapestry} &  & & & \\ \hline
    & & & & \\ \hline  
    ZHT \cite{li2013zht}&   $O(n)$& $O(1)$ &  &Assumes an extremely low churn \\ \hline
    \end{tabularx}
    \caption{The different ratios and their associated DHTs}
    \label{tab:tradeoffs}
\end{table}

\subsubsection{Heterogeneity}
The mechanics behind load balancing assumes nothing about the nature of the hardware it is running on.
The applications that run DHTs, on the otherhand, implicitly assume that that the machines composing the network (and running the application) are heterogeneous.


Can't automatically assign work natively (or can you: end project automatic load balancing DHT?)
Can assign work manually with virtual nodes 



\subsection{The Takeaway}


\begin{itemize}
	\item DHTs are extremely good if your problem is embarrassingly parallel
	\item DHTs are agnostic in terms of what hardware it's running on.
	\item
\end{itemize}
\subsubsection{The consequences of the Properties}
So what are the consequences of these properties?
\begin{itemize}
    \item DHTs can use constistant hashing supplemented by virtual nodes to efficiently load-balance.
	\item DHTs are highly resilient to damage and can handle abnormally high rates of disruption.  This is extremely desirable in a DHT
	\item X is a desirable property in a network for distributed computing by
	\item 
\end{itemize}


\subsubsection{The consequences of the Properties}


\section{Different or subproblem: Certain DHTs are better at one application than another due to differences}
\subsection{Design Differences Impacts}
\subsection{Geometries}
\subsection{Routing Table Construction}
\subsection{Implementation Differences Impacts}
\paragraph{Recursive or iterative seek}


\subsection{MANETS}
\chapter{Justification and Why I Think It's Cool}

\section{Why DHTs for distributed computing?}

\cite{malkhi2001viceroy} -  Between congestion, cost of join/leaves, and lookup time there are tradeoffs.  
Optimizing for two can be done but has bad cost.
For example, a balanced binary tree has congestion at root.
\subsection{DHTs well understood}
\subsection{DHTs are Highly used for their intended purposed}
\subsubsection{Bittorrent, WoW}
\section{DHTs are being effectively leveraged for other things besides file sharing already}

\subsubsection{PaaS}
\subsubsection{Load Balancing in the cloud}
\subsubsection{Computing is a natural extension}

\chapter{Possible Experiments and Applications}




\section{MapReduce}
Both MapReduce \cite{mapreduce} and DHTs are based on operating over key/value pairs, albeit over different contexts.

MapReduce can utilize a DHT to create a more abstract MapReduce frameworkk

\subsection{Current MapReduce DHT/P2P combos}
Example A: slightly decentralized with coordinating nodes.
Also limap


\subsubsection{ChordReduce}
The idea behind ChordReduce is 

ChordReduce is completely decentralized, no node is more vulnerable than any other.


\subsection{Experiment Description: Comparison of MapReduce paradigm on different DHTs}
This may be enough on its own.
\begin{itemize}
	\item Implement CAN, Pastry, Chord, Kademlia, VHash and ZHT/similar
	\begin{itemize}	
		\item This covers different geometries with different base paremeters
		\item This also necessitates the creation of an extensible DHT framework, something I haven't found yet and may not have been done before (most people implement their DHT or use someone else)
	\end{itemize}
	\item Compare results with each other and MapReduce
	
\end{itemize}



\section{High End Computing}
\subsection{Metadata Management}
\subsection{Robustness}

\subsection{Experiment Description:}

\section{Graph Processing on a DHT}
Lookup Graphlab
\subsection{Embedding}

\subsection{Experiment Description:}
\subsection{Distribute the work for solving a graph on a DHT}
\subsection{Comparison to well established or state of the art methods}



\section{Machine Learning Problems on A DHT}
\subsubsection{Bayesian Learning}
\subsection{Experiment Description:}



\section{DHTs as a volunteer Platform}
\subsection{Experiment Description}
Implement and compare to Boinc.



\section{Resources}
\subsection{Planetlab}
\subsection{Local Cluster}

\chapter{DHT Background}



\bibliography{notes}
\bibliographystyle{ieeetr}
\end{document}
