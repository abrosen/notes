\documentclass[10pt,letterpaper,twoside]{report}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage{algorithm}
\usepackage{algpseudocode} 
\usepackage[english]{babel}

%\usepackage{venturis2}


\title{Proposal}
\author{Andrew Rosen}
\begin{document}

 

\maketitle
\setcounter{tocdepth}{4}
\tableofcontents
\newpage

\begin{abstract}
Distributed Hash Tables (DHTs) are protocols and frameworks used by peer-to-peer (P2P) systems.
They are used as the organizational backbone for many P2P file-sharing systems due to their scalability, fault-tolerance, and load-balancing properties.
These same properties are highly desirable in a distributed computing environment, especially one that wants to use heterogeneous components.
DHTs can be used not only as the framework to build a P2P file-sharing service, but a P2P distributed computing platform.

Our framework is a completely decentralized framework for organizing heterogeneous units for distributed computation.
It incorporates a load-balancing algorithm that is capable of injecting additional nodes during runtime to speed up existing jobs.
This algorithm also provides a means of redistributed the load among existing workers during runtime.

%applications
Unlike Hadoop and similar MapReduce frameworks, our framework can be used both in both the context of a datacenter or as part of a P2P computing platform.  
This opens up new possibilities for building platforms to distributed computing problems.
By utilizing the load-balancing algorithm, a datacenter could easily leverage additional P2P resources at runtime on an as-needed basis.
Our framework also allows MapReduce-like or distributed machine learning platforms to be easily deployed in a greater variety of contexts.

\end{abstract}

\chapter{Introduction}

% % % layout
% % % Distributed Computing Challenges
% % % Qualities of DHTs
% % % Hypothesis = These Problems + These qualiteies -> solution
% % % Framework of what these solutions are and what they can do

Distributed Computing is well understood to be the approach to take to solve large problems.  
Many problems can be broken up into multiple parts that can be solved simultaneously, yielding a much quicker result than a single working attacking the problem.
However, there are two broad obstacles in distributed computing.

%Is this 1 problem or two
The first is figuring out how the mechanics of efficiently distributing a problem to multiple workers and asynchronously coordinating their effort.  
The second is creating and maintaining the computation platform itself.


Some specific challenges are:
\begin{description}
	\item[Scalability] - Distributed computing platforms should not be completely static; if the platform would be improved by the addition of a new resource, it should be possible to add that resource.  
	The addition of new workers in a distributed computing framework should be a minimally disruptive process.
    This ties into the network's fault tolerance
	\item[Load-Balancing]  This is one of the most important issues to consider when creating a framework for distributed computing. 
    How do you split up a problem then distribute it so that no single worker is under or over-utilized?
    Failing that, how do you minimize the imbalance in work?
    Is there a way to do so at run time?
    \item[Fault Tolerance]  Even in a network that is expected to remain static for long periods of time, the platform still has to deal with failure.  
    In a centralized environment, hardware failures are common given enough machines.
    We want our platform to gracefully handle failures during runtime and be able to quickly reassign work to other workers.
    In addition, the network should be equally graceful in handling the introduction of new nodes during runtime.
\end{description}

Fortunately, these challenges are not unique to distributed computing but are also obstacles in distributed file storage.  
In particular, distributed file storage applications that utilize Distributed Hash Tables are designed to handle these particular challenges.
\section{Distributed Hash Tables}
%\section{DHTs are better for distributed computing under many circumstances}
Distributed Hash Tables (DHTs) are traditionally used as the backbone of structured Peer-to-Peer (P2P) file-sharing applications.
The largest such application by far is Bittorrent \cite{bittorrent}, which is built using Mainline DHT \cite{mainline},  a  derivative of Kademlia \cite{kademlia}.
The number of users on Bittorrent ranges from 15 million to 27 million users daily, with a turnover of 10 million users a day \cite{mainlineMeasure}.

Most research on DHTs assumes that they will be used in the context of a large P2P file-sharing application  (or at least, an application \textit{potentially} incorporating millions of nodes).
This lends the DHT to having particular qualities.
The network must be able to handle members joining and leaving arbitrarily.
The resulting application must be agnostic towards hardware.
The network must be decentralized and split whatever burden there is equally among its members.

In other words, distributed hash tables provide scalability, load-balancing, robustness, and heterogeneity to an application.
More recent applications have examined leveraging these qualities, since these qualities are desirable in many different frameworks.
For example, one paper \cite{Mateescu2011440} used a DHT as the name resolution layer of a large distributed database.
Research has also been done in using DHTs as an organizing mechanism in distributed machine learning \cite{liparameter}. 


I describe each of the aforementioned qualities and their ramifications below in sections \ref{subsec:ft}, \ref{subsec:lb}, \ref{subsec:scalability}, and \ref{subsec:hetero} .
While these properties are  be individually enumerated, they are greatly intertwined and the division between their impacts can be somewhat arbitrary.

\subsection{Robustness and Fault-Tolerance}
\label{subsec:ft}
One of the most important assumptions of DHTs is that they are deployed on a non-static network.
DHTs need to be built to account for a high level what is called \textit{churn}.  
Churn refers to the disruption of routing caused by the constant joining and leaving of nodes.
This is mitigated by a few factors.

First, the network is decentralized, with no single node acting as a single point of failure.
This is accomplished by each node in the routing table having a small portion of the both the routing table and the information stored on the DHT (see the Load Balancing property below).

Second is that each DHT has an inexpensive maintenance processes that mitigates the damage caused by churn.
DHTs often integrate a backup process into their protocols so that when a node goes down, one of the neighbors can immediately assume responsibility.
The join process also causes disruption to the network, as affected nodes have adjust their peerlists to accommodating the joiner. 

The last property is that the hash algorithm used to distribute content evenly across the network(again see load balancing) also distributes nodes evenly across the DHT.  
This means that nodes in the same geographic region occupy vastly different positions in the keyspace.  
If an entire geographic region is affected by a network outage, this damage is spread evenly across the DHT, which can be handled.

This property is the most important, as it deals with failure of entire sections of the network, rather than a single node.
Recent research in using DHTs for High End Computing \cite{li2013zht} shows what can happen if we remove this assumption by placing the network that is almost completely static.

The fault tolerance mechanisms in DHTs also provide near constant availability for P2P applications.
The node that is responsible for a particular key can always be found, even when numerous failures or joins occur \cite{chord}.

\subsection{Load Balancing}
\label{subsec:lb}
All Distributed Hash Tables use some kind of consistent hashing algorithm to associate nodes and file identifiers with keys.  
These keys are generated by passing the identifiers into a hash function, typically SHA-160.
The chosen hash function is typically large enough to avoid hash collisions and generates keys in a uniform manner. 
The result of this is that as more nodes join the network, the distribution of nodes in the keyspace becomes more uniform, as does the distribution of files.

However, because this is a random process, it is highly unlikely that each node will be spread evenly throughout the network.
This appears to be a weakness, but can be turned into an advantage in heterogeneous systems by using \textit{virtual nodes} \cite{godfrey2005heterogeneity} \cite{dynamo}.
When a node joins the network, it joins not at one position, but multiple virtual positions in the network \cite{dynamo}.
Using virtual nodes allows load-balance optimization in a heterogeneous network; more powerful machines can create more virtual nodes and handle more of the overall responsibility in the network.

DeCandia et al\. discussed various load balancing techniques that were tested on Dynamo \cite{dynamo}.  
Each node was assigned a certain number of tokens and the node would create a virtual node for each token.
The realization DeCandia et al\. had was that there was no reason to use the same scheme for data partitioning and data placement.
DeCandia et al\. introduced two new strategies which work off assigning nodes equally sized partitions.


% HELP
Under these schemes, each virtual node maps to an ID as before, but the patitions each node is responsible for are equally sized\footnote{Need help here}.

\subsubsection{Heterogeneity}
\label{subsec:hetero}
Heterogenity presents a challenge for load balancing DHTs due to conflicting assumptions and goals. 
DHTs assume that members are usually going to be varied in hardware, while the load-balancing process defined in DHTs treats each node equally.
It is much simpler to treat each node as equal unit.
In other words, DHTs support heterogeneity, but do not attempt to exploit it.

This doesn't mean that heterogeneity cannot be exploited
Nodes can be given addition responsibilities manually, by running multiple instances of the P2P application on the same machine or creating more virtual nodes.
However, this is not a feasible option for any kind of truly decentralized system and would need to be done automatically.
There is no well-known mechanism to that exists to automatically  allocate virtual nodes on the fly \footnote{citation needed, although this can be similar to IRM}. 
A few options present themselves.  % and are discuessed in \ref{}
%add the above if the below is moved

%This might be the wrong place for this
One is to use adapt a request tracking mechanism, such as what is used in IRM, except instead of tracking file requests, it tracks requests that are directed to a particular (real) node. 
If a particular (real) node receives an inordinate amount of requests, the node doing the detecting suggests that the node obtain another token/create another virtual node.
Another strategy is to use the preference lists/successor predecessor lists, and observe the distribution of the workload, adjusting the virtual nodes based on that. 

Dynamic load balancing may not be essential to P2P file-sharing applications, but is absolutely essential to any kind of P2P distributed computation.
In our ChordReduce experiments, we observed that just approximating dynamic load-balancing by simulating high levels of churn noticeably improved results\footnote{We found this by accident, just by testing the network's fault tolerance in regards to a high level of churn}.



%While most DHTs follow this scheme, there is some minor variation on how keys are generated.
%Some DHTs can or do use geographic information to generate keys.
%In VHash, keys are not static, but move according to a simplified spring model.

\subsection{Scalability}
\label{subsec:scalability}
In order to maintain scalability, a DHT has to ensure that as the network grows larger:

\begin{itemize}
    \item Churn does not have a disproportionate overhead.  
    For example, in a 1000 node network, a joining or leaving node will affect only an extremely small subset of these nodes.
    \footnote{We will see that this requirement can  be  relaxed in  very specific cases \cite{li2013zht}.}
    \item Lookup request speeds (usually measured in hops) grow by a much smaller amount, possibly not at all.
\end{itemize}

Using consistent hashing allows the network to scale up incrementally, adding one node at a time \cite{dynamo}.
In addition, each join operation has minimal impact on the network, since a node affects only its immediate neighbors on a join operation.
Similarly, the only nodes that need to react to a node leaving are its neighbors.
This is almost instantaneous if the network is using backups.
Other nodes can be notified of the missing node passively through maintenance or in response to a lookup.

There have been multiple proposed strategies for tackling scalability, and it is these strategies which play the greatest role in driving the variety of DHT architectures. 
Each DHT must strikes a balance between memory cost of the peerlist and lookup time. 
The vast majority of DHTs choose to use $\lg(N)$ sized routing tables and  $\lg(n)$ hops \footnote{log n or log N, matters}. 
Chapter \ref{chapter:background} discusses these tradeoffs in greater detail and how they affect the each DHT.





%\section{Different or subproblem: Certain DHTs are better at one application than another due to differences}
%\subsection{Design Differences Impacts}
%\subsection{Geometries}
%\subsection{Routing Table Construction}
%\subsection{Implementation Differences Impacts}
%\paragraph{Recursive or iterative seek}


\section{Hypothesis:  problems in distributed computing + solutions =  dissertation topic}
Distributed computing platforms need to be scalable, fault-tolerant, and load balancing.
In addition, the ability to incorporate heterogeneous hardware is a definite benefit.
Distributed Hash Tables can provide an application with all of these qualities.
P2P applications have been using DHTs for large-scale distributed file sharing applications for years now and are particularly effective.


I propose that DHTs can be used to create P2P distributed computing platforms that are completely decentralized.
Rather than keys being assigned to some data, we can assign keys to tasks and automatically distribute those tasks to the responsible nodes
There would be no need for some central coordinator or scheduler.

A successful DHT based computing platform would need to address the problem of dynamic load-balancing.
This is currently an unsolved problem and if an application can dynamically reassign work to nodes added at runtime, this opens up new options for resource management.
If a computation  is running too slow, new nodes can be added to the network  during runtime or idle nodes can boot up more virtual nodes (now that I think of it this is two different but highly related problems: internal and external).


The next chapter will delve into how DHTs work and examine specific DHTs.
The remainder of the paper will then discuss the work I plan on doing to demonstrate the viability of using DHTs for distributed computing.


%Add these bullets to the above paragraph
%\begin{itemize} 
%	\item DHTs can use consistent hashing supplemented by virtual nodes to efficiently load-balance.
%	The larger the network grows, the more evenly distributed the load becomes. 
%	\item DHTs are highly resilient to damage and can handle abnormally high rates of disruption.  
%	This is extremely desirable in any kind of distributed application %
%	\item Large-scale P2P file sharing applications have been using DHTs for a long time and
%    \item DHTs are extremely good if your problem is embarrassingly par
%    \item Heterogeneity
%\end{itemize}



\chapter{Background}
\label{chapter:background}

DHTs have been a vibrant area of research for the past decade, with some of the concepts dating further back.
Numerous DHTs have been developed over the years.
This is partly because the process of designing DHTs involves making tradeoffs, with no choice being strictly better than any other.

% % %unified terminology
The large number of DHTs have lead many papers use different terms to describe congruent elements of DHTs, as some terms may make sense only in one context.
Since this paper will cover multiple DHTs that would use different terms,  I've created a unified terminology:


\begin{enumerate}
    \item[key] -  The identifier generated by a hash function corresponding to a unique\footnote{Unique with extremely high probability. SHA-1, which generates 160-bit hashes, is typically used as a hashing algorithm.} node or file.
    \item[ID] - The ID is a key that corresponds to a particular node.  
    The ID of a node and the node itself are referred to interchangeably.
    In this paper, I try to refer to nodes by their ID and files by their keys.
	\item[Peer]  - Another active member on the network.  
    For this section, we assume that all peers are different pieces of hardware.
	\item[Peerlist] -  The set of all peers that a node knows about.  
    This is sometimes referred to as the \textit{routing table}, but certain DHTs \cite{tapestry} \cite{pastry} overload the terminology.
    Any table or list of peers is a subset of the entire peerlist.
	\item[Neighbors] - The subset of peers that are ``closest/adjacent'' to the node in the keyspace, according to the DHT's metric.  In a 1-dimensional ring, such a Chord \cite{chord}, this is the node's \textit{predecessor(s)} and \textit{successor(s)}.
	\item[Fingers] - The subset of the peerlist that the node is not adjacent to.  
    These are sometimes referred to as long-hops or shortcuts.
	\item[Root Node] - The node responsible for a particular key. 
	\item[Successor] -  Alternate name for the root node. 
	The successor of a node is the neighbor that will assume a nodes responsibilities if that node leaves. 
    \item[$n$ nodes] -  The number of nodes in the network.
    
\end{enumerate}

Similarly, All DHTs perform the same operations with minor variation.
\begin{enumerate}
	\item[\texttt{lookup(key)}] - This operation finds the root node of \texttt{key}.
	Almost every operation on a DHT needs to leverage the \texttt{lookup} operation in some way.
	\item[\texttt{put(key,value)}] - Stores \texttt{value} at the root node of \texttt{key}.
	Unless otherwise specified, \texttt{key} is assumed be the hashkey of \texttt{value}.
	This assumption is broken in Tapestry.
	\item[\texttt{get(key)}] - This operates like lookup, except the context is to return the value stored by a \texttt{put}.
	This is a subtle difference, since one could \texttt{lookup(key)} and ask it directly.
	However, many implementations use backup operations and caching which will store multiple copies of the value along the network
	If we don't care which node returns the value mapped with key, or if it is a backup,  we can express it with \texttt{get}.
	\item[\texttt{delete(key, value)}] - This is self-explanatory.  Typically, DHTs do not worry about key deletion and leave that option to the specific application.
    When DHTs do address the issue, they often assume that stored key-value pairs have a specified time-to-live, after which they are automatically removed.
\end{enumerate}

On the local level, each node has to be able to \textit{join }and perform maintenance on itself.
\begin{enumerate}
	\item[\texttt{join()}]  The join process encompasses two steps.
    First, the joining node needs to initialize its peerlist. 
    It doesn't necessarily need a complete peerlist the moment it joins, but it must initialize one. 
    Second, the joining node needs to inform other nodes of its existence.
    \item[Maintenance]  Maintenance procedures generally are either \textit{active} or \textit{lazy}.
    In active maintenance, peers are periodically pinged and are replaced when they are no longer detected.
    Lazy maintenance assumes that peers in the peerlist are healthy until they prove otherwise, in which case they are either replaced immediately.
    In general, lazy maintenance is used on everything, while active maintenance is only used on neighbors\footnote{check this statement for consistency}.
    
\end{enumerate}

When analyzing the DHTs in this chapter, we look at the overlay's geometry, the peerlist, the \texttt{lookup} function, and how fault-tolerance is performed in the DHTs.
We assume that nodes never politely leave the network but always abruptly fail, since a \texttt{leave()} operation is fairly trivial and has minimal impact.


\section{Chord}
%Chord \cite{chord} is a P2P protocol for file sharing and distributed storage that guarantees a high probability $\log_{2} n$ lookup time for a particular node or file in the network. 
%It is highly fault-tolerant to node failures and churn, the constant joining and leaving of nodes.  It scales extremely well and the network requires little maintenance to handle individual nodes.  

Chord \cite{chord} is the archetypal ring-based DHT and it is impossible to create a new ring-based DHT without making some comparison to Chord.
It is notable due its straightforward routing, its rules which make ownership of keys very easy to sort out, and the large number of derivatives.


\subsection*{Peerlist and Geometry}
Chord is a 1-dimensional modular ring in which all messages travel in one direction - upstream, hopping from one node to another with a greater ID until it wraps around.
Each member of the network and the data stored is hashed to a unique $m$-bit key or ID, corresponding to one of the $2^m$ locations on a ring. 

A node in the network is responsible for all the data with keys upstream from its predecessor's ID, up through and including its own ID.  
If a node is responsible for some key, it is referred to being the root or successor of that key.

Lookup and routing is performed by recursively querying nodes upstream.
However, querying only neighbors would take $O(n)$ time to lookup a key.


To speedup lookups, each node maintains a table of $m$ shortcuts to other peers, called the \textit{finger table}.
The $i$th entry of a node $n$'s finger table corresponds to the node that is the successor of the key $n+2^{i-1} \mod 2^m $.  
During a lookup,  nodes query the finger that is closest to the sought key without going past it, until it is received by the root node.
Each hop essentially cuts the search space for a key in half.
This provides Chord with a highly scalable $\log_2(n)$ lookup time for any key \cite{chord}, with an average $\frac{1}{2}O(\log_{2}(n))$ number of hops.

Besides the finger tables, the peerlist includes a list of $s$ neighbors in each direction for fault tolerance.
This brings the total size of the peerlist to $log_{2}(2^{m})  + 2 \cdot s =  m  + 2 \cdot s$, assuming the entries are distinct.

\subsection*{Joining}
To join the network, node $n$ first asks $n'$ to find \texttt{successor($ n $)}. 
Node $n$ uses the information to set his successor, and maintenance will inform the other nodes of $n$'s existence.
Meanwhile, $n$ will takeover some of the keys that his successor was responsible for.

\subsection*{Fault Tolerance}
Robustness in the network is accomplished by having nodes backup their contents to their $s$ immediate successors, the closest nodes upstream. 
This is done because when a node leaves the or fail, the most immediate successor would be responsible for the keys.
In the case of multiple nodes failing all at once, having a successor list makes it extremely unlikely that any given stored value will be lost.

As nodes enter and leave the ring, the nodes use their maintenance procedures to guide them into the right place and repair any links with failed nodes.  
The process takes $O(\lg^{2}(n))$ messages.
Full details on Chord's maintenance cycle can be found here \cite{chord}.

%\subsection*{Security}
%An Eclipse attack compromises a DHT by poisoning the routing tables of nodes, such that friendly nodes can only communicate with malicious nodes \cite{dhtsec}.
%Because 





\section{Kademlia}
Kademlia \cite{kademlia}  is perhaps the most well known and definately the most widely DHT, as a modified version of Kademlia (Mainline DHT) is useds as the backbone of the Bittorrent protocol.
The motivation of Kademlia was to create a way for nodes to incorporate peerlist updates with each query made.

%(the security ramifications of gossip based routing tables being ignored, I suppose).

\subsection*{Peerlist and Geometry}
Like Chord, Kademlia uses $m$-bit keys for nodes and files.
However, Kademlia utilizes a binary tree-based structure, with the nodes acting as the leaves of the tree.
Distance between any two nodes in the tree  is calculated by XORing their IDs.
The XOR distance metric means that distances are symmetric, which is not the case in Chord.

%A node's location in the tree given by the shortest unique prefix of its ID.   
%For each bit in the prefix, there would be a subtree which does not contain that node.  
%Kademlia guarantees that the node will know at least one node in each of these subtrees.

Nodes in Kademlia maintain information about the network using a routing table that contains  $m$ lists, called $k$-buckets.
For each $k$-bucket contains up to $k$ nodes that are distance $2^i$ to $2^{i+1}$, where $0 \leq i < m$.
In other words, each $k$-bucket corresponds to a subtree of the network not containing the node.

Each $k$-bucket is maintained by a least recently seen eviction algorithm that skips live nodes.
Whenever the node receives a message, it adds the sender's info to the tail of the corresponding $k$-bucket.
If that info already exists, the info is moved to the tail.

If the $k$-bucket is full, the node starts pinging nodes in the list, starting at the head.
As soon as a node fails to respond, that node is evicted from the list to make way for the new node at the tail.

If there are no modifications to a particular $k$-bucket after a long period of time, the node does a \texttt{refresh} on the $k$-bucket.
A refresh is a \texttt{lookup} of a random key in that $k$-bucket.


%(If I'm an eclipse attacker, I just keep spamming messages of different IDs, but with my own ip address and port info, or with sybils)

\subsection*{Lookup}
In most DHTs, \texttt{lookup(key)} sends a single message and returns the information  of a single node.
The \texttt{lookup} operation in Kademlia differs in both respects:  \texttt{lookup} is done in parallel and each node receiving  a \texttt{lookup(key)} returns the $k$ closest nodes to \texttt{key} it knows about.


A \texttt{lookup(key)} operation begins with the seeking node sending lookups in parallel to the $\alpha$ nodes from the appropriate $k$-bucket.
Each of theses $\alpha$ nodes will asynchronously return the $k$ closest nodes it knows closest to \texttt{key}.
As lookups return their results, the node continue to send lookups until no new nodes\footnote{If a file being stored on the network is the objective, the \texttt{lookup} will also terminate if a node reports having that file.} are found.  

\footnote{I would argue that this lookup operation is not recursive as claimed by the paper, but iterative, since the initiator sends all the messages.}

\subsection*{Joining}
A joining node starts with a single contact and then performs a \textit{lookup} operation on it's own ID.
Each step of the \textit{lookup} operation yields new nodes for the joining node's peerlist and informs other nodes of its existence.
Finally, the joining node performs a \texttt{refresh} on each $k$-bucket farther away than the closest node it knows of.




\subsection*{Fault-Tolerance}
Nodes actively republish each file stored on the network each hour by rerunning the \texttt{store} command.  
To avoid flooding the network, two optimizations are used.

First if a node receives a \texttt{store} on a file it is holding, it assumes $k-1$ other nodes got that same command and resets the timer for that file.
This means only one node republishes a file each hour.
Secondly, \texttt{lookup} is not performed during a republish.


Additional fault tolerance is provided by the nature of the \texttt{store(data)} operation, which \texttt{puts }the file in the $k$ closest nodes to the key.
However, there's very little in the way of frequent and active maintenance other than what occurs during \texttt{lookup} and the other operations.


%\subsubsection*{Caching}
%Files are cached during a \texttt{get} operation and stored at the closest node that the seeker found that did not return a result.
%The cache has an expiration 










\section{CAN}
REREAD, APPARENTLY A COUPLE OF WAYS TO define NEIGHBORHOOD


Unlike the previous DHTs presented in this chapter, the Content Addressable Network (CAN) \cite{can} works in a $d$-dimensional torus, with the entire coordinate space divided among members.
A node is responsible for the keys  that fall within the ``zone'' that it owns.
Each key is hashed into some point within the geometric space.

\subsection*{Peerlist and Geometry}
CAN uses an exceptionally simple peerlist consisting only of neighbors.  
Every node in the CAN network is assigned a geometric region in the coordinate space and each node maintains a routing table consisting each node that borders the node's region.

The size of the routing table is a function of the number of dimensions, $O(d)$. 
The lower bound on the routing tables size in a populated network (eg, a network with at least $2d$ nodes) is $\Omega(2d)$.  
This is obtained by looking at each axis, where there is at least one node bordering each end of the axis.
The size of the routing table can grow as more nodes join and the space gets further divided; however, maintenance algorithms prevent the regions from becoming too fragmented.


\subsection*{Lookup}
As previously mentioned, each node maintains a routing table corresponding to their neighbors, those nodes it shares a face with.
Each hop forwards the lookup to the neighbor closest to the destination, until it comes to the responsible node.
In a space that is evenly divided among $n$ nodes, this simple routing scheme uses only $2 \cdot d$ space while giving average path length of $\frac{d}{4}\cdot n^{\frac{1}{d}}$.
The overall lookup time of in CAN is bounded by $O(n^{\frac{1}{d}})$ hops\footnote{Around the same time CAN was being developed, Kleinberg was doing research into small world networks \cite{kleinberg2000small}.  
He proved similar properties for lattice networks with a single shortcut.  What makes this network remarkable is lack of shortcuts.}.

% fault tolerence in routing
If a node encounters a failure during lookup, the node simply chooses the next best path.
However, if lookups occur before a node can recover from damage inflicted by churn, it is possible for the greedy lookup to fail.
The fallback method is to use an expanding ring search until a candidate is found, which recommences greedy forwarding.

\subsection*{Joining}
Joining works by splitting the geometric space between nodes.  
If node $n$ with location $P$ wishes to join the network, it contacts a member of the node to find the node $m$ currently responsible for location $P$.
Node $n$ informs $m$ that it is joining and they divide $m$'s region such that each becomes responsible for half.

Once the new zones have been defined, $n$ and $m$ create its routing table from $m$ and its former neighbors.
These nodes are then informed of the changes that just occurred and update their tables.
As a result, the join operation affects only $O(d)$ nodes.  
More details on this splitting process can be found in CAN's original paper \cite{can}.

\subsection*{Repairing}
A node in a DHT that notifies its neighbors that its leaves usually has minimal impact to the  network and in this is true for most cases in CAN.
A leaving node, $f$, simply hands over its zone to one of its neighbors of the same size, which merges the two zones together.
Minor complications occur if this is not possible, when there is no equally-sized neighbor. 
In this case, $f$ hands its zone to its smallest neighbor, who must wait for this fragmentation to be fixed.



Unplanned failures are also relatively simple to deal with.
Each node broadcasts a heartbeat to its neighbors, containing its and its neighbors' coordinates.
If a node fails to hear a heartbeat from $f$ after a number of cycles, it assumes $f$ must have failed and begins a \texttt{takeover} countdown.
When this countdown ends, the node broadcasts\footnote{This message is sent to all of $f$'s neighbors;  I assume that nodes must keep track of their neighbors' neighbors.} a \texttt{takeover} message in an attempt to claim $f$'s space.
This message contains the node's volume.
When a node receives a \texttt{takeover} message, it either cancels the countdown or, if the node's zone is smaller than the broadcaster's, responds with its own \texttt{takeover}.

The general rule of thumb for node failures in CAN is that the neighbor with the smallest zone takes over the zone of the failed node.
This rule leads to quick recoveries that affect only $O(d)$ nodes, but requires a zone reassignment algorithm to remove the fragmentation that occurs from \texttt{takeovers}.

To summarize, a failed node is detected almost immediately, and recovery occurs extremely quickly, but fragmentation must be fixed by a maintenance algorithm.




%As mentioned earlier in the text, Ratnasamy et al. \cite{can}  also present the concept own using landmarks to choose coordinates, rather than a has function.
%Each node measures the round-trip time (RTT) to each to of the $m$ landmarks, which yields one of $m!$ permutations.
%The keyspace is partitioned into $m!$ regions, each corresponding to one of the orderings.  
%A joining node now chooses a random location from the region corresponding to its landmark ordering.






%\subsection*{Design Improvements}
%Ratnasamy et al.\ identified a number of improvements that could be made to CAN \cite{can}.
%Some of these improvements have already be explored in Chapter 1.

%One modification to the system is increasing the number of dimensions in the coordinate space.
%Increasing $d$ improves fault tolerance and reduces path length.

%One concept Ratnasamy et al.\  introduces is the idea of multiple coordinate spaces existing simultaneously, called \textit{realities}. 
%Each object in the DHT exists at a different set of coordinates for each reality simultaneously.
%So a node might have coordinates $(x_0,y_0,z_0)$ in one reality, while having coordinates $(x_1,y_1,z_1)$ in another.
%Independent sets of neighbors for each reality yield different the overall topologies and mappings of keys to nodes.
%Multiple realities increase the cost of maintenance and routing table sizes, but provide greater fault tolerance and greater data availability.

%A final modification is to allow multiple nodes shares the same zone (ie zones don't necessarily split as a result of a join operation).    


\section{Pastry}

%Addressing - 128 bit ID, 0 to $2^{128} -1$, assigned randomly using hash.   but thought of as base $2^{b}$ numbers (typically b=4).  
Pastry \cite{pastry} and Tapestry \cite{tapestry} are extremely similar use a prefix-based routing mechnism introduced by Plaxton et al.\ \cite{plaxton1999accessing}.
In Pastry and Tapestry, each key is encoded as a base $ 2^{b} $ number (typically $b=4$ in Pastry, which yields easily readable hexadecimal).
The resulting peerlist best resembles a hypercube topology \cite{induced}, with each node being a vertice of the hypercube.

One notable feature of Pastry is the incorperation of a proximity metric.
The peerlist uses IDs that are close to the node according to this metric.

\subsection*{Peerlist}
Pastry's peerlist consists of three components: the routing table, a leaf set, and a neighborhood set.  
The routing table consists of $\log_{2^{b}}(n)$ rows with $2^{b} -1 $ entries per row. 
The $i$th level of the routing table correspond to the peers with that match first $i$ digits of the example nodes ID.

Thus, the 0th row contains peers which don't share a common prefix with the node, the 1st row contains those that share a length 1 common prefix, the 2nd a length 2 common prefix, etc.  
Since each ID is a base $2^b$ number, there is one entry for each of the $2^{b} -1 $ possible differences.   

For example, let is consider a node 05AF in system where $b = 4$ and the hexadecimal keyspace ranges from $0000$ to FFFF.
\begin{itemize}
    \item 1322 would be an appropriate peer for the 1st entry of level 0.
    \item 0AF2 would be an appropriate peer for the 10th\footnote{0 is the 0th level.  It's easier that way.} entry of level 1.
    \item 09AA would be an appropriate peer for the 9th entry of level 1.	
    \item 05F2 would be an appropriate peer for the 2nd entry of level 3.
\end{itemize}


The leaf set is used to hold the $L$ nodes with the numerically closest IDs;  half of it for smaller IDs and half for the larger.
A typical value for $L$ is $2^b$ or $2^{b+1}$.
The leaf set is used for routing when the destination key is close to the current node's ID.
The neighborhood set contains the $L$ closest nodes, as defined by some proximity metric.  
It, however, is generally not used for routing.  



\subsection*{Lookup}
The \texttt{lookup} operation is a fairly straightforward recursive operation.
The \texttt{lookup(key)} terminates when the \texttt{key} is falls within the range of the leaf set, which are the nodes \emph{numerically} closest to the current node.
In this case, the destination will be one of the leaf set, or the current node.

If the destination node is not immediately apparent, the node uses its routing table to select the next node.
The node looks at the length $l$ shared prefix,  at examines the $l$th row of its routing table.
From this row, the \texttt{lookup} continues with the entry that matches at least another digit of the prefix.
In the case that this entry does not exist or has failed, the \texttt{lookup} continues from the closest ID chosen from the entire peerlist.
This process is described by Algorithm \ref{PastryLookup}.
Lookup is expected  to take $\lceil \log_{2^{b}} \rceil $, as each hop along the routing table reduces the search space by $\frac{1}{2^{b}}$.
 
\begin{algorithm}
    \caption{Pastry lookup algorithm}
    \label{PastryLookup}
    \begin{algorithmic}
        \State Let $L$ be the routing  
        \Function{Lookup}{$key$}
            \If {$key$ is in the range of the leaf set }
            	\State destination is closest ID in the leaf set or self
            \Else
            	\State $next\gets$ entry from routing table that matches $\geq 1$ more digit
            	\If {$next \neq null$}
                	\State forward to $next$
            	\Else
                	\State forward to the closest ID from the entire peerlist
                \EndIf
                
            \EndIf
        \EndFunction
    \end{algorithmic}
\end{algorithm}

\subsection*{Joining}
To join the network, node $J$ sends a \texttt{join} message to $A$, some node that is close according to the proximity metric.
The \texttt{join} message is forwarded along like a \texttt{lookup} to the root of $X$, which we'll call $root$.
Each node that received the \texttt{join} sends a copy of the their peerlist to $J$.

The leaf set is constructed from copying $root$'s leaf set, while $i$th row in the routing table routing table is copied from the $i$th node contacted along the \texttt{join}.
The neighborhood set is copied from $A$'s neighborhood set, as \texttt{join} predicates that $A$ be close to $J$.
This means $A$'s neighborhood set would be close to $A$. 

After the joining node creates its peerlist, it sends a copy to each node in the table, who then can update their routing tables.  
The cost of a \texttt{join} is $O(log_{2}^{b} n)$ messages,  with  a constant  coefficient  of $3*2^{b}$




\subsection*{Fault Tolerance}
Pastry lazily repairs its leaf set and routing table.
When node from the leaf set fails, the node contacts the node with largest or smallest ID (depending if the failed node ID was smaller or larger respectively) in the leaf set.
That node returns a copy of its leaf set, and the node replaces the failed entry.
If the failed node is in the routing table, the node contacts a node with an entry in the same row as the failed node for a replacement.

Members of the neighborhood set are actively checked.
If a member of the neighborhood set is unresponsive, the node obtains a copy of another entry's neighborhood set and repairs from a selection.



%\subsection*{Proximity Metric}
%Pastry's goal is to minimize the ``distance'' messages travel, where distance can be defined by some metric, typically the number of hops.
%The leaf set is the  of nodes closest to the node in the keyspace.  
%The neighborhood set is the of nodes closest to the node according to the distance metric. 
%Guarantees routing time is  $<\log n$ in typical operation.  
%Guarantees eventual delivery except when half of the leaf nodes fail simultaneously.






%\section{Tapestry}
%Tapestry \cite{tapestry} is based off the same prefix-based lookup \cite{prr} as Pastry \cite{pastry} and the peerlist and lookup operation share many similarities.
%Tapestry views itself more as a DOLR \cite{dolr}.
%This essentially means that it is a distributed key-based lookup system like a DHT \cite{hildrum2004distributed}, but with some subtle differences at the abstract level which manifest as large %implementation changes.
%The essential difference here is that Tapestry has servers \textit{publish} records/objects on the network, which direct lookups to the server.  
%The assumption here seems to be that the servers, not the responsible node, serve the actual data.  
%DHTs care or don't care on an application to application basis whether keys are associated with records or content. 






% ``Small'' routing tables
\section{Symphony and Small World Routing}
Symphony  \cite{symphony} is a 1$d$ ring-based DHT similar to Chord \cite{chord}, but is constructed using the properties of small world networks \cite{kleinberg2000small}.
Small world networks owe their name to a phenomena observed by psychologists in the late 1960's. 

Subjects in experiments were to route a postal message to a target person; for example the wife of a Cambridge divinity student in one experiment and a Boston stockbroker in another \cite{milgram1967small}.
The messages were only to be routed by forwarding them to a friend they thought most likely to know the target.
Of the messages that successfully made their way to the destination, the average path length from a subject to a participant was only 5 hops.  

This lead to research investigating creating a network with randomly distributed links, but with a efficient lookup time.
Kleinberg \cite{kleinberg2000navigation} showed that in a 2-dimensional lattice network, nodes could route messages in $O(\log^{2}n)$ hops using only their neighbors and a single randomly chosen\footnote{Randomly chosen from a specified distribution.} finger.
In other words,$O(\log^{2}n)$ lookup is achievable with a $O(1)$ sized routing table.

\subsection*{Peerlist}
Rather than the 2-dimensional lattice used by Kleinberg, Symphony uses a 1-dimensional ring\footnote{This is technically a 1-dimensional lattice.} like Chord.
Symphony assigns $m$-bit keys to the modular unit interval $ [0,1)$, instead of using a keyspace ranging from 0 to $2^{n} - 1$.
This location is found  with $\frac{hashkey}{2^{m}}$.
This is arbitrary from a design standpoint, but makes choosing from a random distribution simpler. 

Nodes know both their immediate predecessor and successor, much like in Chord.
Nodes also keep track of some  $k \geq 1$ fingers, but, unlike in Chord, these fingers are chosen at random.
These fingers are chosen from a probability distribution corresponding to the expression $e^{ln(n) + (rand48() - 1.0)}$, where $n$ is the number of nodes in the network and \texttt{rand48()} is a C function that generates a random float?double between 0.0 and 1.0.
Because $n$ os difficult to compute due to the changing nature of P2P networks, each node uses an approximation is used based on the distance between themselves and their neighbors.

A final feature of note is that links in Symphony are bidirectional.
Thus, if a node creates a finger to a peer, that peer creates a, so nodes in Symphony have a grand total of $2k$ fingers.
%(although it gets me thinking, is there any advantage/statistical properties   that could be exploited by making the space monic)


\subsection*{Joining and Fault Tolerance}
The joining and fault tolerance processes in Symphony are extremely straightforward.
After determining its ID, a joining node asks a member to find the root node for its ID.
The joining node integrates itself in between its predecessor and successor and then randomly generates its fingers.

Failures of immediate neighbors are handled by use of successor and predecessor lists
Failures for fingers are handled lazily and are replaced by another randomly generated link when a failure is detected.

% Large Routing Tables
\section{ZHT}
One of the major assumptions of DHT design is that churn is a significant factor, which requires constant maintenance to handle.
A consequence of this assumption is that nodes only store a small subset of the entire network to route to.
Storing the entire network is not scalable for the vast majority of distributed systems due to bandwidth constraints and communication overhead incurred by the constant joining and leaving of nodes.

In a system that does not expect churn, the memory and bandwidth costs for each node to keep a full copy of the routing table are minimal.
An example of this would be a data center or a cluster built for higher-performance computing, where churn would overwhelmingly be the result of hardware failure, rather than users quitting.




\subsection*{Peerlist}



%Approximated routing tables
\section{VHash}


\section{Summary}

% % % table
% Perhaps geometries should be included?
% be sure to include join and leave costs.

\begin{table}[h]
	\small
	\centering
	\begin{tabularx}{\textwidth}{ |X|X|X|X|X| }
		\hline
		% Add join leave cost, avgerages and maxs
		DHT & Routing Table Size & Lookup Time & Join/Leave & Comments \\ \hline  
		
		Chord \cite{chord} & $O(\log n)$, maximum $m +2s$ & $O(\log n)$, avg $\frac{1}{2} \log n)$  &  $<O(\log n^{2})$ total messages& $m$  = keysize in bits, $s$ is neighbors in 1 direction  \\ \hline
		
		Kademlia \cite{kademlia} & $O(\log n)$, maximum $m\ \cdot k$ & $\lceil \log n\rceil) + c$ & $O(\log(n))$& This is without considering optimization   \\ \hline
		CAN \cite{can} & $\Omega(2d)$ & $O(n^{\frac{1}{d}})$, average $\frac{d}{4}\cdot n^{\frac{1}{d}}$ & Affects $O(d)$ nodes & $d$ is the number of dimensions \\ \hline
		
		Plaxton-based DHTs, Pastry \cite{pastry}, Tapestry \cite{tapestry} & $O(\log_{\beta} n)$ & & &  NodeIDs are base $\beta$ numbers \\ \hline
		
        Symphony \cite{symphony}& $2k + 2$&   average $O(\frac{1}{k} \log^{2} n )$ & $O(\log^{2} n)$ messages,  constant $<1$ &  $k \geq 1$, fingers are chosen at random\\ \hline  
		
        ZHT \cite{li2013zht}&   $O(n)$& $O(1)$ &  $O(n)$ & Assumes an extremely low churn \\ \hline
        
        VHash & $\Omega(3d+1) + O((3d+1)^{2})$ & $O(\log^{2}n)$ hops & $3d + 1$ & approximates regions, hops are based least latency\\ \hline
	\end{tabularx}
	\caption{The different ratios and their associated DHTs}
	\label{tab:tradeoffs}
\end{table}

% % % Specific DHTs



\subsection{DHTs as a volunteer Platform}
Rather than rely on a centralized administrative source,


Decentralized resource discovery.
The system is organized using a P2P system built on Brunet \cite{brunet}.





PonD \cite{lee2012pond}



\chapter{Possible Experiments and Applications}

\section{Why DHTs for distributed computing?}

\cite{malkhi2001viceroy} -  Between congestion, cost of join/leaves, and lookup time there are tradeoffs.  
Optimizing for two can be done but has bad cost.
For example, a balanced binary tree has congestion at root.


\section{MapReduce}

%copied from CHRONUS
Google's MapReduce \cite{mapreduce} paradigm has rapidly become an integral part in the world of data processing and is capable of efficiently executing numerous Big Data programming and data-reduction tasks.  
By using MapReduce, a user can take a large problem, split it into small, equivalent tasks and send those tasks to other processors for computation.  
The results are sent back to the user and combined into one answer.  
MapReduce has proven to be an extremely powerful and versatile tool, providing the framework for using distributed computing to solve a wide variety of problems, such as distributed sorting and creating an inverted index \cite{mapreduce}. 

At it's core, MapReduce \cite{mapreduce} is a system for process key/value pairs, a that statement that equally describes DHTs.
However, MapReduce operates over a different set of assumptions \cite{hadoopAssumptions} than DHTs.
MapReduce platforms are highly centralized and tend to have single points of failure\cite{shvachko2010hadoop} as a result.   
A centralized design assumes that the network is relatively unchanging and does not usually have mechanisms to handle node failure during execution or, conversely, cannot speed up the execution of a job by adding additional workers on the fly.
Finally deploying these systems and developing programs for them has an extremely steep learning curve.

If we make MapReduce operate under the same assumptions as a DHT, we have effectively further abstracted the MapReduce paradigm and created a system that can operate both in a traditional large datacenter or as part of a P2P network.
The system would be highly resistant to failures at any point, scalable, and automatically load-balance. 
The administrator can add any number of heterogeneous nodes to the system to get it operate.

\subsection{Current MapReduce DHT/P2P combos}
There have been a few implementations combining MapReduce with a P2P framework, in varying capacities.  
I will present two here, as well as my own implementation, ChordReduce.

\subsubsection{P2P-MapReduce}
Marozzo et al. \cite{marozzo2012p2p} investigated the issue of fault tolerance in centralized MapReduce architectures such as Hadoop.  
They focused on creating a new P2P based MapReduce architecture built on JXTA called P2P-MapReduce.  
P2P-MapReduce is designed to be more robust at handling node and job failures during execution.

Rather than use a single master node, P2P-MapReduce employs multiple master nodes, each responsible for some job.  
If one of those master nodes fails, another will be ready as a backup to take its place and manage the slave nodes assigned to that job.  
This avoids the single point of failure that Hadoop is vulnerable to. Failures of the slave nodes are handled by the master node responsible for it.

Experimental results were gathered via simulation and compared P2P-MapReduce to a centralized framework. 
Their results showed that while P2P-MapReduce generated an order of magnitude more messages than a centralized approach, the difference rapidly began to shrink at higher rates of churn.  
When looking at actual amounts of data being passed around the network, the bandwidth required by the centralized approach greatly increased as a function of churn, while the distributed approach again remained relatively static in terms of increased bandwidth usage. 
They concluded that P2P-MapReduce would, in general, use more network resources than a centralized approach. 
However, this was an acceptable cost as the P2P-MapReduce would lose less time from node and job failures \cite{marozzo2012p2p}.

\subsubsection{Parralel Processing Framework on a P2P System}
Lee et al.'s work \cite{leemap} draws attention to the fact that a P2P network can be much more than a way to distribute files and demonstrates how to accomplish different tasks using Map and Reduce functions over a P2P network.  
Rather than using Chord, Lee et al. used Symphony \cite{symphony}, another DHT protocol with a ring topology.  
To run a MapReduce job over the Symphony ring, a node is selected by the user to effectively act as the master.  
This ad-hoc master then performs a bounded broadcast over a subsection the ring.  
Each node repeats this broadcast over a subsection of that subsection, resulting in a tree with the first node at the top.  

Map tasks are disseminated evenly throughout the tree and their results are reduced on the way back up to the ad-hoc master node.  
This allows the ring to disseminate Map and Reduce tasks without the need for a coordinator responsible for distributing these tasks and keeping track of them, unlike Hadoop.  
Their experimental results showed that the latency experienced by a centralized configuration is similar to the latency experienced in a completely distributed framework.





\subsubsection{ChordReduce}
ChordReduce is designed as a more abstract framework for MapReduce, able to run on any arbitrary distributed configuration.
ChordReduce leverages the features of distributed hash tables to handle distributed file storage, fault tolerance, and lookup.  
ChordReduce  was designed to ensure that no single node is a point of failure and that there is no need for any node to coordinate the efforts of other nodes during processing.  



\subsection{Experiment Description: Comparison of MapReduce paradigm on different DHTs}
In order to test MapReduce over a DHT, I will do the following:
\begin{itemize}
	\item Implement CAN \cite{can}, Pastry \cite{pastry}, Chord \cite{chord}, Kademlia \cite{kademlia}, VHash, and ZHT \cite{li2013zht} /similar
	\begin{itemize}	
		\item This covers different geometries with different base parameters.
		\item This also necessitates the creation of an extensible DHT framework.
		\item  The DHT should be extended with more powerful search functionality (see distributed database below), and built-in policies for virtual nodes.
		
	\end{itemize}
	\item Compare results with each other and a traditional MapReduce platform, such as Hadoop.
	\item Certain DHTs may be better suited to different problem formulation
	
\end{itemize}



\section{High End Computing}
PonD?
\subsection{Metadata Management}
\subsection{Robustness}

\subsection{Experiment Description:}

\section{Graph Processing on a DHT}
Lookup Graphlab
\subsection{Embedding}

\subsection{Experiment Description:}
\subsection{Distribute the work for solving a graph on a DHT}
\subsection{Comparison to well established or state of the art methods}



\section{Machine Learning Problems on A DHT}


\subsubsection{Bayesian Learning}
\subsection{Experiment Description:}
Take MapReduce machine learning algorithm

\section{Distributed Databases}


Want to find all files that match the criteria?

Simple: Find all files with ``author = John Smith''.  Idiot solution, assign ``author = John Smith'' a hash key,  it's value is a file with all the files with the (that doesn't scale) 


Complex: Processing database queries.   Find all files with age < 20 and niceness >12


\section{Semiautomagic Load Balancing}
What: Automagic load balancing.  One of two possiblilities:  inject new node into region or create new virtual node in region. 
Requires Where, when, and how/which

\begin{itemize}
    \item Where: Can be answered with Sybil selection.
    \item When:  Can be answered with IRM for hot spots.  Can be answered with neighbor monitoring
    \item How:  The remaining peace
    \item Symphony demonstrates how to estimate $n$
\end{itemize}
\section{Resources}
\subsection{Planetlab}
\subsection{Local Cluster}



\bibliography{notes}
\bibliographystyle{ieeetr}
\end{document}
