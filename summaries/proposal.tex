\documentclass[10pt,letterpaper,twoside]{report}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage[english]{babel}


%\usepackage{venturis2}

\title{Proposal}
\author{Andrew Rosen}
\begin{document}


\maketitle
\setcounter{tocdepth}{4}
\tableofcontents
\newpage

\begin{abstract}
Distributed Hash Tables (DHTs) are protocols and frameworks used by peer-to-peer (P2P) systems.
They are used as the organizational backbone for many P2P file-sharing systems due to their scalability, fault-tolerance, and load-balancing properties.
These same properties are highly desirable in a distributed computing environment, especially one that wants to use heterogeneous components.
DHTs can be used not only as the framework to build a P2P file-sharing service, but a P2P distributed computing platform.

Our framework is a completely decentralized framework for organizing heterogeneous units for distributed computation.
It incorporates a load-balancing algorithm that is capable of injecting additional nodes during runtime to speed up existing jobs.
This algorithm also provides a means of redistributed the load among existing workers during runtime.

%applications
Unlike Hadoop and similar MapReduce frameworks, our framework can be used both in both the context of a datacenter or as part of a P2P computing platform.  
This opens up new possibilities for building platforms to distributed computing problems.
By utilizing the load-balancing algorithm, a datacenter could easily leverage additional P2P resources at runtime on an as-needed basis.
Our framework also allows MapReduce-like or distributed machine learning platforms to be easily deployed in a greater variety of contexts.

\end{abstract}

\chapter{Introduction}

% % % layout
% % % Distributed Computing Challenges
% % % Qualities of DHTs
% % % Hypothesis = These Problems + These qualiteies -> solution
% % % Framework of what these solutions are and what they can do

Distributed Computing is well understood to be the approach to take to solve large problems.  
Many problems can be broken up into multiple parts that can be solved simultaneously, yielding a much quicker result than a single working attacking the problem.
However, there are two broad obstacles in distributed computing.

%Is this 1 problem or two
The first is figuring out how the mechanics of efficiently distributing a problem to multiple workers and asynchronously coordinating their effort.  
The second is creating and maintaining the computation platform itself.


Some specific challenges are:
\begin{description}
	\item[Scalability] - Distributed computing platforms should not be completely static; if the platform would be improved by the addition of a new resource, it should be possible to add that resource.  
	The addition of new workers in a distributed computing framework should be a minimally disruptive process.
    This ties into the network's fault tolerance
	\item[Load-Balancing]  This is one of the most important issues to consider when creating a framework for distributed computing. 
    How do you split up a problem then distribute it so that no single worker is under or over-utilized?
    Failing that, how do you minimize the imbalance in work?
    Is there a way to do so at run time?
    \item[Fault Tolerance]  Even in a network that is expected to remain static for long periods of time, the platform still has to deal with failure.  
    In a centralized environment, hardware failures are common given enough machines.
    We want our platform to gracefully handle failures during runtime and be able to quickly reassign work to other workers.
    In addition, the network should be equally graceful in handling the introduction of new nodes during runtime.
\end{description}

Fortunately, these challenges are not unique to distributed computing but are also obstacles in distributed file storage.  
In particular, distributed file storage applications that utilize Distributed Hash Tables are designed to handle these particular challenges.
\section{Distributed Hash Tables}
%\section{DHTs are better for distributed computing under many circumstances}
Distributed Hash Tables (DHTs) are traditionally used as the backbone of structured Peer-to-Peer (P2P) file-sharing applications.
The largest such application by far is Bittorrent \cite{bittorrent}, which is built using Mainline DHT \cite{mainline},  a  derivative of Kademlia \cite{kademlia}.
The number of users on Bittorrent ranges from 15 million to 27 million users daily, with a turnover of 10 million users a day \cite{mainline}.

Most research on DHTs assumes that they will be used in the context of a large P2P file-sharing application  (or at least, an application \textit{potentially} incorporating millions of nodes).
This lends the DHT to having particular qualities.
The network must be able to handle members joining and leaving arbitrarily.
The resulting application must be agnostic towards hardware.
The network must be decentralized and split whatever burden there is equally among its members.

In other words, distributed hash tables provide scalability, load-balancing, robustness, and heterogeneity to an application.
More recent applications have examined leveraging these qualities, since these qualities are desirable in many different frameworks.
For example, one paper \cite{Mateescu2011440} used a DHT as the name resolution layer of a large distributed database.
Research has also been done in using DHTs as an organizing mechanism in distributed machine learning \cite{liparameter}. 


I decribe each of the aformentioned qualities and their ramifications below in sections \ref{subsec:ft}, \ref{subsec:lb}, \ref{subsec:scalability}, and \ref{subsec:hetero} .
While these properties are  be individually enumerated, they are greatly intertwined and the division between their impacts can be somewhat arbitrary.

\subsection{Robustness and Fault-Tolerance}
\label{subsec:ft}
One of the most important assumptions of DHTs is that they are deployed on a non-static network.
DHTs need to be built to account for a high level what is called \textit{churn}.  
Churn refers to the disruption of routing caused by the constant joining and leaving of nodes.
This is mitigated by a few factors.

First, the network is decentralized, with no single node acting as a single point of failure.
This is accomplished by each node in the routing table having a small portion of the both the routing table and the information stored on the DHT (see the Load Balancing property below).

Second is that each DHT has an inexpensive maintenance processes that mitigates the damage caused by churn.
DHTs often integrate a backup process into their protocols so that when a node goes down, one of the neighbors can immediately assume responsibility.
The join process also causes disruption to the network, as affected nodes have adjust their peerlists to accommodating the joiner. 

The last property is that the hash algorithm used to distribute content evenly across the network(again see load balancing) also distributes nodes evenly across the DHT.  
This means that nodes in the same geographic region occupy vastly different positions in the keyspace.  
If an entire geographic region is affected by a network outage, this damage is spread evenly across the DHT, which can be handled.

This property is the most important, as it deals with failure of entire sections of the network, rather than a single node.
Recent research in using DHTs for High End Computing \cite{li2013zht} shows what can happen if we remove this assumption by placing the network that is almost completely static.

The fault tolerance mechanisms in DHTs also provide near constant availability for P2P applications.
The node that is responsible for a particular key can always be found, even when numerous failures or joins occur \cite{chord}.

\subsection{Load Balancing}
\label{subsec:lb}
All Distributed Hash Tables use some kind of consistent hashing algorithm to associate nodes and file identifiers with keys.  
These keys are generated by passing the identifiers into a hash function, typically SHA-160.
The chosen hash function is typically large enough to avoid hash collisions and generates keys in a uniform manner. 
The result of this is that as more nodes join the network, the distribution of nodes in the keyspace becomes more uniform, as does the distribution of files.

However, because this is a random process, it is highly unlikely that each node will be spread evenly throughout the network.
This appears to be a weakness, but can be turned into an advantage in heterogeneous systems by using \textit{virtual nodes} \cite{godfrey2005heterogeneity} \cite{dynamo}.
When a node joins the network, it joins not at one position, but multiple virtual positions in the network \cite{dynamo}.
Using virtual nodes allows load-balance optimization in a heterogeneous network; more powerful machines can create more virtual nodes and handle more of the overall responsibility in the network.

DeCandia et al\. discussed various load balancing techniques that were tested on Dynamo \cite{dynamo}.  
Each node was assigned a certain number of tokens and the node would create a virtual node for each token.
The realization DeCandia et al\. had was that there was no reason to use the same scheme for data partitioning and data placement.
DeCandia et al\. introduced two new strategies which work off assigning nodes equally sized partitions.


% HELP
Under these schemes, each virtual node maps to an ID as before, but the patitions each node is responsible for are equally sized\footnote{Need help here}.

\subsubsection{Heterogeneity}
\label{subsec:hetero}
Heterogenity presents a challenge for load balancing DHTs due to conflicting assumptions and goals. 
DHTs assume that members are usually going to be varied in hardware, while the load-balancing process defined in DHTs treats each node equally.
It is much simpler to treat each node as equal unit.
In other words, DHTs support heterogeneity, but do not attempt to exploit it.

This doesn't mean that heterogeneity cannot be exploited
Nodes can be given addition responsibilities manually, by running multiple instances of the P2P application on the same machine or creating more virtual nodes.
However, this is not a feasible option for any kind of truly decentralized system and would need to be done automatically.
There is no well-known mechanism to that exists to automatically  allocate virtual nodes on the fly \footnote{citation needed, although this can be similar to IRM}. 
A few options present themselves.  % and are discuessed in \ref{}
%add the above if the below is moved

%This might be the wrong place for this
One is to use adapt a request tracking mechanism, such as what is used in IRM, except instead of tracking file requests, it tracks requests that are directed to a particular (real) node. 
If a particular (real) node receives an inordinate amount of requests, the node doing the detecting suggests that the node obtain another token/create another virtual node.
Another strategy is to use the preference lists/successor predecessor lists, and observe the distribution of the workload, adjusting the virtual nodes based on that. 

Dynamic load balancing may not be essential to P2P file-sharing applications, but is absolutely essential to any kind of P2P distributed computation.
In our ChordReduce experiments, we observed that just approximating dynamic load-balancing by simulating high levels of churn noticeably improved results\footnote{We found this by accident, just by testing the network's fault tolerance in regards to a high level of churn}.



%While most DHTs follow this scheme, there is some minor variation on how keys are generated.
%Some DHTs can or do use geographic information to generate keys.
%In VHash, keys are not static, but move according to a simplified spring model.

\subsection{Scalability}
\label{subsec:scalability}
In order to maintain scalability, a DHT has to ensure that as the network grows larger:

\begin{itemize}
    \item Churn does not have a disproportionate overhead.  
    For example, in a 1000 node network, a joining or leaving node will affect only an extremely small subset of these nodes.
    \footnote{We will see that this requirement can  be  relaxed in  very specific cases \cite{li2013zht}.}
    \item Lookup request speeds (usually measured in hops) grow by a much smaller amount, possibly not at all.
\end{itemize}

Using consistent hashing allows the network to scale up incrementally, adding one node at a time \cite{dynamo}.
In addition, each join operation has minimal impact on the network, since a node affects only its immediate neighbors on a join operation.
Similarly, the only nodes that need to react to a node leaving are its neighbors.
This is almost instantaneous if the network is using backups.
Other nodes can be notified of the missing node passively through maintenance or in response to a lookup.

There have been multiple proposed strategies for tackling scalability, and it is these strategies which play the greatest role in driving the variety of DHT architectures. 
Each DHT must strikes a balance between memory cost of the peerlist and lookup time. 
The vast majority of DHTs choose to use $\lg(N)$ sized routing tables and  $\lg(n)$ hops \footnote{log n or log N, matters}. 
Chapter \ref{chapter:background} discusses these tradeoffs in greater detail and how they affect the each DHT.





%\section{Different or subproblem: Certain DHTs are better at one application than another due to differences}
%\subsection{Design Differences Impacts}
%\subsection{Geometries}
%\subsection{Routing Table Construction}
%\subsection{Implementation Differences Impacts}
%\paragraph{Recursive or iterative seek}


\section{Hypothesis:  problems in distributed computing + solutions =  dissertation topic}
Distributed computing platforms need to be scalable, fault-tolerant, and load balancing.
In addition, the ability to incorporate heterogeneous hardware is a definite benefit.
Distributed Hash Tables can provide an application with all of these qualities.
P2P applications have been using DHTs for large-scale distributed file sharing applications for years now and are particularly effective.


I propose that DHTs can be used to create P2P distributed computing platforms that are completely decentralized.
Rather than keys being assigned to some data, we can assign keys to tasks and automatically distribute those tasks to the responsible nodes
There would be no need for some central coordinator or scheduler.

A successful DHT based computing platform would need to address the problem of dynamic load-balancing.
This is currently an unsolved problem and if an application can dynamically reassign work to nodes added at runtime, this opens up new options for resource management.
If a computation  is running too slow, new nodes can be added to the network  during runtime or idle nodes can boot up more virtual nodes (now that I think of it this is two different but highly related problems: internal and external).


The next chapter will delve into how DHTs work and examine specific DHTs.
The remainder of the paper will then discuss the work I plan on doing to demonstrate the viability of using DHTs for distributed computing.


%Add these bullets to the above paragraph
%\begin{itemize} 
%	\item DHTs can use consistent hashing supplemented by virtual nodes to efficiently load-balance.
%	The larger the network grows, the more evenly distributed the load becomes. 
%	\item DHTs are highly resilient to damage and can handle abnormally high rates of disruption.  
%	This is extremely desirable in any kind of distributed application %
%	\item Large-scale P2P file sharing applications have been using DHTs for a long time and
%    \item DHTs are extremely good if your problem is embarrassingly par
%    \item Heterogeneity
%\end{itemize}



\chapter{Background}
\label{chapter:background}

DHTs have been a vibrant area of research for the past decade, with some of the concepts dating further back.
Numerous DHTs have been developed over the years.
This is partly because the process of designing DHTs involves making tradeoffs, with no choice being strictly better than any other.

% % %unified terminology
The large number of DHTs have lead many papers use different terms to describe congruent elements of DHTs, as some terms may make sense only in one context.
Since this paper will cover multiple DHTs that would use different terms,  I've created a unified terminology:


\begin{enumerate}
    \item[key] -  The identifier generated by a hash function corresponding to a unique\footnote{Unique with extremely high probability. SHA-1, which generates 160-bit hashes, is typically used as a hashing algorithm.} node or file.
    \item[ID] - The ID is a key that corresponds to a particular node.  
    The ID of a node and the node itself are referred to interchangeably.
    In this paper, I try to refer to nodes by their ID and files by their keys.
	\item[Peer]  - Another active member on the network.  
    For this section, we assume that all peers are different pieces of hardware.
	\item[Peerlist] -  The set of all peers that a node knows about.  
    This is sometimes referred to as the \textit{routing table}, but certain DHTs \cite{tapestry} \cite{pastry} overload the terminology.
    Any table or list of peers is a subset of the entire peerlist.
	\item[Neighbors] - The subset of peers that are ``closest/adjacent'' to the node in the keyspace, according to the DHT's metric.  In a 1-dimensional ring, such a Chord \cite{chord}, this is the node's \textit{predecessor(s)} and \textit{successor(s)}.
	\item[Fingers] - The subset of the peerlist that the node is not adjacent to.  
    These are sometimes referred to as long-hops or shortcuts.
	\item[Root Node] - The node responsible for a particular key. 
	\item[Successor] -  Alternate name for the root node. 
	The successor of a node is the neighbor that will assume a nodes responsibilities if that node leaves. 
    \item[$n$ nodes] -  The number of nodes in the network.
    
\end{enumerate}

Similarly, All DHTs perform the same operations with minor variation.
\begin{enumerate}
	\item[\texttt{lookup(key)}] - This operation finds the root node of \texttt{key}.
	Almost every operation on a DHT needs to leverage the \texttt{lookup} operation in some way.
	\item[\texttt{put(key,value)}] - Stores \texttt{value} at the root node of \texttt{key}.
	Unless otherwise specified, \texttt{key} is assumed be the hashkey of \texttt{value}.
	This assumption is broken in Tapestry.
	\item[\texttt{get(key)}] - This operates like lookup, except the context is to return the value stored by a \texttt{put}.
	This is a subtle difference, since one could \texttt{lookup(key)} and ask it directly.
	However, many implementations use backup operations and caching which will store multiple copies of the value along the network
	If we don't care which node returns the value mapped with key, or if it is a backup,  we can express it with \texttt{get}.
	\item[\texttt{delete(key, value)}] - This is self-explanatory.  Typically, DHTs do not worry about key deletion and leave that option to the specific application.
    When DHTs do address the issue, they often assume that stored key-value pairs have a specified time-to-live, after which they are automatically removed.
\end{enumerate}
When analyzing the DHTs in this chapter, we look at the overlay's geometry, the peerlist, the \texttt{lookup} function, and how fault-tolerance is performed in the DHTs.
We assume that nodes never politely leave the network but always abruptly fail, since a \texttt{leave()} operation is fairly trivial and has minimal impact.


\section{Chord}
%Chord \cite{chord} is a P2P protocol for file sharing and distributed storage that guarantees a high probability $\log_{2} n$ lookup time for a particular node or file in the network. 
%It is highly fault-tolerant to node failures and churn, the constant joining and leaving of nodes.  It scales extremely well and the network requires little maintenance to handle individual nodes.  

Chord \cite{chord} is the archetypal ring-based DHT and it is impossible to create a new ring-based DHT without making some comparison to Chord.
It is notable due its straightforward routing, its rules which make ownership of keys very easy to sort out, and the large number of derivatives.


\subsection*{Peerlist and Geometry}
Chord is a 1-dimensional modular ring in which all messages travel in one direction - upstream, hopping from one node to another with a greater ID until it wraps around.
Each member of the network and the data stored is hashed to a unique $m$-bit key or ID, corresponding to one of the $2^m$ locations on a ring. 

A node in the network is responsible for all the data with keys upstream from its predecessor's ID, up through and including its own ID.  
If a node is responsible for some key, it is referred to being the root or successor of that key.

Lookup and routing is performed by recursively querying nodes upstream.
However, querying only neighbors would take $O(n)$ time to lookup a key.


To speedup lookups, each node maintains a table of $m$ shortcuts to other peers, called the \textit{finger table}.
The $i$th entry of a node $n$'s finger table corresponds to the node that is the successor of the key $n+2^{i-1} \mod 2^m $.  
During a lookup,  nodes query the finger that is closest to the sought key without going past it, until it is received by the root node.
Each hop essentially cuts the search space for a key in half.
This provides Chord with a highly scalable $\log_2(n)$ lookup time for any key \cite{chord}, with an average $\frac{1}{2}O(\log_{2}(n))$ number of hops.

Besides the finger tables, the peerlist includes a list of $s$ neighbors in each direction for fault tolerance.
This brings the total size of the peerlist to $log_{2}(2^{m})  + 2 \cdot s =  m  + 2 \cdot s$, assuming the entries are distinct.

\subsection*{Joining}
To join the network, node $n$ first asks $n'$ to find \texttt{successor($ n $)}. 
Node $n$ uses the information to set his successor, and maintenance will inform the other nodes of $n$'s existence.
Meanwhile, $n$ will takeover some of the keys that his successor was responsible for.

\subsection*{Fault Tolerance}
Robustness in the network is accomplished by having nodes backup their contents to their $s$ immediate successors, the closest nodes upstream. 
This is done because when a node leaves the or fail, the most immediate successor would be responsible for the keys.
In the case of multiple nodes failing all at once, having a successor list makes it extremely unlikely that any given stored value will be lost.

As nodes enter and leave the ring, the nodes use their maintenance procedures to guide them into the right place and repair any links with failed nodes.  
The process takes $O(\lg^{2}(n))$ messages.
Full details on Chord's maintenance cycle can be found here \cite{chord}.

%\subsection*{Security}
%An Eclipse attack compromises a DHT by poisoning the routing tables of nodes, such that friendly nodes can only communicate with malicious nodes \cite{dhtsec}.
%Because 





\section{Kademlia}
Kademlia \cite{kademlia}  is perhaps the most well known and definately the most widely DHT, as a modified version of Kademlia (Mainline DHT) is useds as the backbone of the Bittorrent protocol.
The motivation of Kademlia was to create a way for nodes to incorporate peerlist updates with each query made.

%(the security ramifications of gossip based routing tables being ignored, I suppose).

\subsection*{Peerlist and Geometry}
Like Chord, Kademlia uses $m$-bit keys for nodes and files.
However, Kademlia utilizes a binary tree-based structure, with the nodes acting as the leaves of the tree.
Distance between any two nodes in the tree  is calculated by XORing their IDs.
The XOR distance metric means that distances are symmetric, which is not the case in Chord.

%A node's location in the tree given by the shortest unique prefix of its ID.   
%For each bit in the prefix, there would be a subtree which does not contain that node.  
%Kademlia guarantees that the node will know at least one node in each of these subtrees.

Nodes in Kademlia maintain information about the network using a routing table that contains  $m$ lists, called $k$-buckets.
For each $k$-bucket contains up to $k$ nodes that are distance $2^i$ to $2^{i+1}$, where $0 \leq i < m$.
In other words, each $k$-bucket corresponds to a subtree of the network not containing the node.

Each $k$-bucket is maintained by a least recently seen eviction algorithm that skips live nodes.
Whenever the node receives a message, it adds the sender's info to the tail of the corresponding $k$-bucket.
If that info already exists, the info is moved to the tail.

If the $k$-bucket is full, the node starts pinging nodes in the list, starting at the head.
As soon as a node fails to respond, that node is evicted from the list to make way for the new node at the tail.

If there are no modifications to a particular $k$-bucket after a long period of time, the node does a \texttt{refresh} on the $k$-bucket.
A refresh is a \texttt{lookup} of a random key in that $k$-bucket.


%(If I'm an eclipse attacker, I just keep spamming messages of different IDs, but with my own ip address and port info, or with sybils)

\subsection*{Lookup}
In most DHTs, \texttt{lookup(key)} sends a single message and returns the information  of a single node.
The \texttt{lookup} operation in Kademlia differs in both respects:  \texttt{lookup} is done in parallel and each node receiving  a \texttt{lookup(key)} returns the $k$ closest nodes to \texttt{key} it knows about.


A \texttt{lookup(key)} operation begins with the seeking node sending lookups in parallel to the $\alpha$ nodes from the appropriate $k$-bucket.
Each of theses $\alpha$ nodes will asynchronously return the $k$ closest nodes it knows closest to \texttt{key}.
As lookups return their results, the node continue to send lookups until no new nodes\footnote{If a file being stored on the network is the objective, the \texttt{lookup} will also terminate if a node reports having that file.} are found.  

\footnote{I would argue that this lookup operation is not recursive as claimed by the paper, but iterative, since the initiator sends all the messages.}

\subsection*{Joining}
A joining node starts with a single contact and then performs a \textit{lookup} operation on it's own ID.
Each step of the \textit{lookup} operation yields new nodes for the joining node's peerlist and informs other nodes of its existence.
Finally, the joining node performs a \texttt{refresh} on each $k$-bucket farther away than the closest node it knows of.




\subsection*{Fault-Tolerance}
Nodes actively republish each file stored on the network each hour by rerunning the \texttt{store} command.  
To avoid flooding the network, two optimizations are used.

First if a node receives a \texttt{store} on a file it is holding, it assumes $k-1$ other nodes got that same command and resets the timer for that file.
This means only one node republishes a file each hour.
Secondly, \texttt{lookup} is not performed during a republish.


Additional fault tolerance is provided by the nature of the \texttt{store(data)} operation, which \texttt{puts }the file in the $k$ closest nodes to the key.
However, there's very little in the way of frequent and active maintenance other than what occurs during \texttt{lookup} and the other operations.


%\subsubsection*{Caching}
%Files are cached during a \texttt{get} operation and stored at the closest node that the seeker found that did not return a result.
%The cache has an expiration 










\section{CAN}
Unlike the previous DHTs presented in this chapter, the Content Addressable Network (CAN) \cite{can} works in a $d$-dimensional torus, with the entire coordinate space divided among members.
A node is responsible for the keys  that fall within the ``zone'' that it owns.
Each key is hashed into some point within the geometric space.

\subsection*{Peerlist and Geometry}
CAN uses an exceptionally simple peerlist consisting only of neighbors.  
Every node in the CAN network is assigned a geometric region in the coordinate space and each node maintains a routing table consisting each node that borders the node's region.

The size of the routing table is a function of the number of dimensions, $O(d)$. 
The lower bound on the routing tables size in a populated network (eg, a network with at least $2d$ nodes) is $\Omega(2d)$.  
This is obtained by looking at each axis, where there is at least one node bordering each end of the axis.
The size of the routing table can grow as more nodes join and the space gets further divided; however, maintenance algorithms prevent the regions from becoming too fragmented.


\subsection*{Lookup}
As previously mentioned, each node maintains a routing table corresponding to their neighbors, those nodes it shares a face with.
Each hop forwards the lookup to the neighbor closest to the destination, until it comes to the responsible node.
In a space that is evenly divided among $n$ nodes, this simple routing scheme uses only $2 \cdot d$ space while giving average path length of $\frac{d}{4}\cdot n^{\frac{1}{d}}$.
The overall lookup time of in CAN is bounded by $O(n^{\frac{1}{d}})$ hops\footnote{Around the same time CAN was being developed, Kleinberg was doing research into small world networks \cite{kleinberg2000small}.  
He proved similar properties for lattice networks with a single shortcut.  What makes this network remarkable is lack of shortcuts.}.

% fault tolerence in routing
If a node encounters a failure during lookup, the node simply chooses the next best path.
However, if lookups occur before a node can recover from damage inflicted by churn, it is possible for the greedy lookup to fail.
The fallback method is to use an expanding ring search until a candidate is found, which recommences greedy forwarding.

\subsection*{Joining}
Joining works by splitting the geometric space between nodes.  
If node $n$ with location $P$ wishes to join the network, it contacts a member of the node to find the node $m$ currently responsible for location $P$.
Node $n$ informs $m$ that it is joining and they divide $m$'s region such that each becomes responsible for half.

Once the new zones have been defined, $n$ and $m$ create its routing table from $m$ and its former neighbors.
These nodes are then informed of the changes that just occurred and update their tables.
As a result, the join operation affects only $O(d)$ nodes.  
More details on this splitting process can be found in CAN's original paper \cite{can}.

\subsection*{Repairing}
A node in a DHT that notifies its neighbors that its leaves usually has minimal impact to the  network and in this is true for most cases in CAN.
A leaving node, $f$, simply hands over its zone to one of its neighbors of the same size, which merges the two zones together.
Minor complications occur if this is not possible, when there is no equally-sized neighbor. 
In this case, $f$ hands its zone to its smallest neighbor, who must wait for this fragmentation to be fixed.



Unplanned failures are also relatively simple to deal with.
Each node broadcasts a heartbeat to its neighbors, containing its and its neighbors' coordinates.
If a node fails to hear a heartbeat from $f$ after a number of cycles, it assumes $f$ must have failed and begins a \texttt{takeover} countdown.
When this countdown ends, the node broadcasts\footnote{This message is sent to all of $f$'s neighbors;  I assume that nodes must keep track of their neighbors' neighbors.} a \texttt{takeover} message in an attempt to claim $f$'s space.
This message contains the node's volume.
When a node receives a \texttt{takeover} message, it either cancels the countdown or, if the node's zone is smaller than the broadcaster's, responds with its own \texttt{takeover}.

The general rule of thumb for node failures in CAN is that the neighbor with the smallest zone takes over the zone of the failed node.
This rule leads to quick recoveries that affect only $O(d)$ nodes, but requires a zone reassignment algorithm to remove the fragmentation that occurs from \texttt{takeovers}.

To summarize, a failed node is detected almost immediately, and recovery occurs extremely quickly, but fragmentation must be fixed by a maintenance algorithm.




%As mentioned earlier in the text, Ratnasamy et al. \cite{can}  also present the concept own using landmarks to choose coordinates, rather than a has function.
%Each node measures the round-trip time (RTT) to each to of the $m$ landmarks, which yields one of $m!$ permutations.
%The keyspace is partitioned into $m!$ regions, each corresponding to one of the orderings.  
%A joining node now chooses a random location from the region corresponding to its landmark ordering.






%\subsection*{Design Improvements}
%Ratnasamy et al.\ identified a number of improvements that could be made to CAN \cite{can}.
%Some of these improvements have already be explored in Chapter 1.

%One modification to the system is increasing the number of dimensions in the coordinate space.
%Increasing $d$ improves fault tolerance and reduces path length.

%One concept Ratnasamy et al.\  introduces is the idea of multiple coordinate spaces existing simultaneously, called \textit{realities}. 
%Each object in the DHT exists at a different set of coordinates for each reality simultaneously.
%So a node might have coordinates $(x_0,y_0,z_0)$ in one reality, while having coordinates $(x_1,y_1,z_1)$ in another.
%Independent sets of neighbors for each reality yield different the overall topologies and mappings of keys to nodes.
%Multiple realities increase the cost of maintenance and routing table sizes, but provide greater fault tolerance and greater data availability.

%A final modification is to allow multiple nodes shares the same zone (ie zones don't necessarily split as a result of a join operation).    








\section{Pastry}

%Addressing - 128 bit ID, 0 to $2^{128} -1$, assigned randomly using hash.   but thought of as base $2^{b}$ numbers (typically b=4).  
Pastry \cite{pastry} and Tapestry \cite{tapestry} are based off of prefix matching.
\cite{plaxton1999accessing}
This creates a hypercube topology \cite{induced}.

\subsection*{Peerlist}
Pastry's peerlist consists of three components: the routing table, a neighborhood set and a leaf set.  
The Routing table consists of $\log(n)$ rows and $b$ columns each. 
The 0th row contains peers which don't share a common prefix with the node.  
The 1st row contains those that share a length 1 common prefix, the 2nd a length 2 common prefix, etc.  
Since each ID is a base $2^b$ number, there is one column for each possible difference.   
The $i$,$j$ entry of the table contains an ID that shares the same first i digits, with digit i+1 having a value of j \footnote{a slot is ``empty'' in each row}.

The neighborhood set holds the ID and IP address of the closest nodes, as defined by some metric.  
It is not used for routing.  
The leaf set is used to hold the nodes with the numerically closest IDs;  half of it for smaller IDs and half for the larger.


\subsection*{Lookup}
Forwarded to (node/peer?) whose shared prefix is longer.  If no one has a better shared prefix than the current node, the message is forwarded to the closest  node.

Lookup takes less that $  \lceil \log_{2^{b}} \rceil $



\subsection*{Joining}
The table is populated at first by a join message to the node responsible for the joining node ID.  
As part of the join message, nodes  along the path send their routing tables.  
After the joining node creates it's routing table, it sends a copy to each node in the table, who then can update their routing tables.   
Node join cost is $O(log_{2}^{b} n)$ messages  with  a constant  coefficient  of $3*2^{b}$




\subsection*{Fault Tolerance}

When a node leaves the network, its neighbor contacts its leaf closest to the failed node  for its leaf table.  That information is used to repair the leaf set.  A failed routing node is replaced with another appropriate node for that slot.  

Who do we actively back up to? 
Pastry is only about routing.
PAST stores a file to the $k$ closest nodes with IDs closest to the file.  
This allows messages to make it to any one of the $k$ nodes that can respond to that file lookup (most likely the closest one to the originator)
%Fault Tolernence
A failed node doesn't delay  routing, because Pastry's routing table allows it to just send to the next closest node.  
Damage to the routing table is replaced by contacting other nodes and requesting a suitable replacement




\subsection*{Metric}
Pastry's goal is to minimize the ``distance'' messages travel, but distance can be defined by some metric, typically the number of hops.
The leaf set is the  of nodes closest to the node in the keyspace.  
The neighborhood set is the of nodes closest to the node according to the distance metric. 
Guarantees routing time is  $<\log n$ in typical operation.  
Guarantees eventual delivery except when half of the leaf nodes fail simultaneously.



\subsection*{Attacks and Vulnerabilities}
Eclipse attack would basically work like this -  when a node asks the malicious one for peer info, the malicious node replies with IDs it makes up on the spot, each bound to it's IP. These IPs would be spread throughout the keyspace so that any malicious value has a good chance of being chosen.





\section{Tapestry}
Tapestry \cite{tapestry} is based off the same prefix-based lookup \cite{prr} as Pastry \cite{pastry} and the peerlist and lookup operation share many similarities.
Tapestry views itself more as a DOLR \cite{dolr}.
This essentially means that it is a distributed key-based lookup system like a DHT \cite{hildrum2004distributed}, but with some subtle differences at the abstract level which manifest as large implementation changes.
The essential difference here is that Tapestry has servers \textit{publish} records/objects on the network, which direct lookups to the server.  
The assumption here seems to be that the servers, not the responsible node, serve the actual data.  
DHTs care or don't care on an application to application basis whether keys are associated with records or content. 




\subsection*{Peerlist}
Node IDs are stored in base-$\beta$, which influences the size of the peerlist and the lookup time.
Nodes maintain a routing table with $\log_{\beta}N$ levels and $\beta$ entries each level.
Each level corresponds to peers with IDs that match the node's ID up to a certain length, with each entry corresponding to a different digit following that prefix.

For example, let is consider a node in system where $\beta = 16 $ and the keyspace ranges from $0000$ to FFFF.
Our example node has the ID 05AF.
Let the $i$th level of the routing table correspond to the peers with that match first $i$ digits of the example nodes ID.
\begin{itemize}
	\item 0AF2 would be an appropriate peer for the 10th\footnote{0 is the 0th level.  It's easier that way.} entry of level 1.
	\item 09AA would be an appropriate peer for the 9th entry of level 1.	
	\item 05F2 would be an appropriate peer for the 2nd entry of level 3.
	\item 1322 would be an appropriate peer for the 1st entry of level 0.
\end{itemize}

In addition to the routing table, each link to a node is bidirectional.


\subsubsection*{Construction and Maintenance}
Need to look at other paper.

\subsection*{Lookup and Publishing}
Tapestry \cite{tapestry} implements a version of the prefix-based lookup introduces by Plaxton et al.\ \cite{prr}. 
However, as a DOLR, lookups in Tapestry differ subtly.
Objects with some $k$ advertised or \textit{published} at the node with the ID closest\footnote{Not explicitly defined, but assumed to be in distance.} to the key.
This node is referred to as the \textit{root node}.

To find the root node for some key $k$, nodes forward lookup requests to the entry in the routing table with the closest ID to $k$. 
This is an extremely simple operation because of the prefix-based organization.

When a node receives a lookup request for key $k$, it looks at the $i$th level of the routing table, where $i$ is the length of the shared prefix between the node's ID and $k$.
If the ID and $k$ match, or the lookup request cannot be forwarded any further, then the node is the root for $k$.

During the publish operation, each node along the lookup to the root node stores a copy of the ?advertisement?. 
These copies can be used as a short-circuit to the advertiser.
It is assumed these copies expire as the publisher is required to periodically republish each key.



\subsection*{Joining}

A joining node $j$ finds the root node for $j_{ID}$, which we will refer to as the parent.
The parent node will share prefix of length $p$ with the joining node.
The parent sends a message  that notifies each node in the network with the same prefix of joining node.
These nodes adjust their routing tables and contact the joining node.
The joining node uses the notified nodes as the basis to start building its routing table and accepts responsibility for root from notified nodes, if necessary.


\subsection*{Repairing}
Most of the fault tolerant features are provided by multiple entries per level  in the routing table and a heartbeat function to periodically check if the root is still live.


%\subsection*{Attacking}
%Need to know more about table construction first.






% ``Small'' routing tables
\section{Symphony and Small World Routing}
Symphony  \cite{symphony} is a 1$d$ ring-based DHT similar to Chord \cite{chord}, but is constructed using the properties of small world networks \cite{kleinberg2000small}.
Small world networks owe their name to a phenomena observed by psychologists in the late 1960's. 
Subjects in experiments were to route a postal message to a target person; for example the wife of a Cambridge divinity student in one experiment and a Boston stockbroker in another \cite{milgram1967small}.
The messages were only to be routed by forwarding them to a friend they thought most likely to know the target.
Of the messages that successfully made their way to the destination, the average path length from a subject to a participant was only 5 hops.  

This lead to research investigating creating a network with randomly distributed links, but maintaining a polylogarithmic lookup.
Kleinberg \cite{kleinberg2000navigation} showed that in a 2-dimensional lattice network, nodes could route messages in $O(\log^{2}n)$ hops using only their neighbors and a single randomly chosen\footnote{Randomly chosen from a specified distribution.} finger.
In other words,$O(\log^{2}n)$ lookup is achievable with a $O(1)$ sized routing table.

\subsection*{Peerlist}
Rather than a 2-dimensional lattice, Symphony uses a 1-dimensional ring.
Symphony assigns $m$-bit keys to the modular unit interval $ [0,1)$, instead of using a keyspace ranging from 0 to $2^{n} - 1$.
This location is found  with $\frac{hashkey}{2^{m}}$.
This is arbitrary from a design standpoint, but makes choosing from a random distribution simpler. 

Nodes know both their immediate predecessor and successo, much like in Chord.
Nodes also keep track of some  $k \geq 1$ fingers, but, unlike in Chord, these fingers are chosen at random.

%(although it gets me thinking, is there any advantage/statistical properties   that could be exploited by making the space monic)


\subsection*{Joining}



\subsection*{Fault Tolerance}
Failures for fingers are handled lazily and are replaced by another randomly generated link when a failure is detected.

% Large Routing Tables
\section{ZHT}
One of the major assumptions of DHT design is that churn is a signifcant factor that requires constant maintenense to handle.



\subsection*{Peerlist}



%Approximated routing tables
\section{VHash}


\section{Summary}

% % % table
% Perhaps geometries should be included?
% be sure to include join and leave costs.

\begin{table}[h]
	\small
	\centering
	\begin{tabularx}{\textwidth}{ |X|X|X|X|X| }
		\hline
		% Add join leave cost, avgerages and maxs
		DHT & Routing Table Size & Lookup Time & Join/Leave & Comments \\ \hline  
		
		Chord \cite{chord} & $O(\log n)$, maximum $m +2s$ & $O(\log n)$, avg $\frac{1}{2} \log n)$  &  $<O(\log n^{2})$ total messages& $m$  = keysize in bits, $s$ is neighbors in 1 direction  \\ \hline
		
		Kademlia \cite{kademlia} & $O(\log n)$, maximum $m\ \cdot k$ & $\lceil \log n\rceil) + c$ & $O(\log(n))$& This is without considering optimization   \\ \hline
		CAN \cite{can} & $\Omega(2d)$ & $O(n^{\frac{1}{d}})$, average $\frac{d}{4}\cdot n^{\frac{1}{d}}$ & Affects $O(d)$ nodes & $d$ is the number of dimensions \\ \hline
		
		Plaxton-based DHTs, Pastry \cite{pastry}, Tapestry \cite{tapestry} & $O(\log_{B} n)$ & & & \\ \hline
		Symphony \cite{symphony}& $2k + 2$&   average $O(\frac{1}{k} \log^{2} n )$ & $O(\log^{2} n)$ messages,  constant $<1$ &  $k \geq 1$, fingers are chosen at random\\ \hline  
		ZHT \cite{li2013zht}&   $O(n)$& $O(1)$ &  $O(n)$ & Assumes an extremely low churn \\ \hline
        VHash & $\Omega(3d+1) + O((3d+1)^{2})$ & $O(\log^{2}n)$ hops & $3d + 1$ & approximates regions, hops are based least latency\\ \hline
	\end{tabularx}
	\caption{The different ratios and their associated DHTs}
	\label{tab:tradeoffs}
\end{table}

% % % Specific DHTs



\subsection{DHTs as a volunteer Platform}
Rather than rely on a centralized administrative source,


Decentralized resource discovery.
The system is organized using a P2P system built on Brunet \cite{brunet}.





PonD \cite{lee2012pond}



\chapter{Possible Experiments and Applications}

\section{Why DHTs for distributed computing?}

\cite{malkhi2001viceroy} -  Between congestion, cost of join/leaves, and lookup time there are tradeoffs.  
Optimizing for two can be done but has bad cost.
For example, a balanced binary tree has congestion at root.


\section{MapReduce}

%copied from CHRONUS
Google's MapReduce \cite{mapreduce} paradigm has rapidly become an integral part in the world of data processing and is capable of efficiently executing numerous Big Data programming and data-reduction tasks.  
By using MapReduce, a user can take a large problem, split it into small, equivalent tasks and send those tasks to other processors for computation.  
The results are sent back to the user and combined into one answer.  
MapReduce has proven to be an extremely powerful and versatile tool, providing the framework for using distributed computing to solve a wide variety of problems, such as distributed sorting and creating an inverted index \cite{mapreduce}. 

At it's core, MapReduce \cite{mapreduce} is a system for process key/value pairs, a that statement that equally describes DHTs.
However, MapReduce operates over a different set of assumptions \cite{hadoopAssumptions} than DHTs.
MapReduce platforms are highly centralized and tend to have single points of failure\cite{shvachko2010hadoop} as a result.   
A centralized design assumes that the network is relatively unchanging and does not usually have mechanisms to handle node failure during execution or, conversely, cannot speed up the execution of a job by adding additional workers on the fly.
Finally deploying these systems and developing programs for them has an extremely steep learning curve.

If we make MapReduce operate under the same assumptions as a DHT, we have effectively further abstracted the MapReduce paradigm and created a system that can operate both in a traditional large datacenter or as part of a P2P network.
The system would be highly resistant to failures at any point, scalable, and automatically load-balance. 
The administrator can add any number of heterogeneous nodes to the system to get it operate.

\subsection{Current MapReduce DHT/P2P combos}
There have been a few implementations combining MapReduce with a P2P framework, in varying capacities.  
I will present two here, as well as my own implementation, ChordReduce.

\subsubsection{P2P-MapReduce}
Marozzo et al. \cite{marozzo2012p2p} investigated the issue of fault tolerance in centralized MapReduce architectures such as Hadoop.  
They focused on creating a new P2P based MapReduce architecture built on JXTA called P2P-MapReduce.  
P2P-MapReduce is designed to be more robust at handling node and job failures during execution.

Rather than use a single master node, P2P-MapReduce employs multiple master nodes, each responsible for some job.  
If one of those master nodes fails, another will be ready as a backup to take its place and manage the slave nodes assigned to that job.  
This avoids the single point of failure that Hadoop is vulnerable to. Failures of the slave nodes are handled by the master node responsible for it.

Experimental results were gathered via simulation and compared P2P-MapReduce to a centralized framework. 
Their results showed that while P2P-MapReduce generated an order of magnitude more messages than a centralized approach, the difference rapidly began to shrink at higher rates of churn.  
When looking at actual amounts of data being passed around the network, the bandwidth required by the centralized approach greatly increased as a function of churn, while the distributed approach again remained relatively static in terms of increased bandwidth usage. 
They concluded that P2P-MapReduce would, in general, use more network resources than a centralized approach. 
However, this was an acceptable cost as the P2P-MapReduce would lose less time from node and job failures \cite{marozzo2012p2p}.

\subsubsection{Parralel Processing Framework on a P2P System}
Lee et al.'s work \cite{leemap} draws attention to the fact that a P2P network can be much more than a way to distribute files and demonstrates how to accomplish different tasks using Map and Reduce functions over a P2P network.  
Rather than using Chord, Lee et al. used Symphony \cite{symphony}, another DHT protocol with a ring topology.  
To run a MapReduce job over the Symphony ring, a node is selected by the user to effectively act as the master.  
This ad-hoc master then performs a bounded broadcast over a subsection the ring.  
Each node repeats this broadcast over a subsection of that subsection, resulting in a tree with the first node at the top.  

Map tasks are disseminated evenly throughout the tree and their results are reduced on the way back up to the ad-hoc master node.  
This allows the ring to disseminate Map and Reduce tasks without the need for a coordinator responsible for distributing these tasks and keeping track of them, unlike Hadoop.  
Their experimental results showed that the latency experienced by a centralized configuration is similar to the latency experienced in a completely distributed framework.





\subsubsection{ChordReduce}
ChordReduce is designed as a more abstract framework for MapReduce, able to run on any arbitrary distributed configuration.
ChordReduce leverages the features of distributed hash tables to handle distributed file storage, fault tolerance, and lookup.  
ChordReduce  was designed to ensure that no single node is a point of failure and that there is no need for any node to coordinate the efforts of other nodes during processing.  



\subsection{Experiment Description: Comparison of MapReduce paradigm on different DHTs}
In order to test MapReduce over a DHT, I will do the following:
\begin{itemize}
	\item Implement CAN \cite{can}, Pastry \cite{pastry}, Chord \cite{chord}, Kademlia \cite{kademlia}, VHash, and ZHT \cite{li2013zht} /similar
	\begin{itemize}	
		\item This covers different geometries with different base parameters.
		\item This also necessitates the creation of an extensible DHT framework.
		\item  The DHT should be extended with more powerful search functionality (see distributed database below), and built-in policies for virtual nodes.
		
	\end{itemize}
	\item Compare results with each other and a traditional MapReduce platform, such as Hadoop.
	\item Certain DHTs may be better suited to different problem formulation
	
\end{itemize}



\section{High End Computing}
PonD?
\subsection{Metadata Management}
\subsection{Robustness}

\subsection{Experiment Description:}

\section{Graph Processing on a DHT}
Lookup Graphlab
\subsection{Embedding}

\subsection{Experiment Description:}
\subsection{Distribute the work for solving a graph on a DHT}
\subsection{Comparison to well established or state of the art methods}



\section{Machine Learning Problems on A DHT}


\subsubsection{Bayesian Learning}
\subsection{Experiment Description:}
Take MapReduce machine learning algorithm

\section{Distributed Databases}


Want to find all files that match the criteria?

Simple: Find all files with ``author = John Smith''.  Idiot solution, assign ``author = John Smith'' a hash key,  it's value is a file with all the files with the (that doesn't scale) 


Complex: Processing database queries.   Find all files with age < 20 and niceness >12


\section{Semiautomagic Load Balancing}
What: Automagic load balancing.  One of two possiblilities:  inject new node into region or create new virtual node in region. 
Requires Where, when, and how/which

\begin{itemize}
    \item Where: Can be answered with Sybil selection.
    \item When:  Can be answered with IRM for hot spots.  Can be answered with neighbor monitoring
    \item How:  The remaining peace
    \item Symphony demonstrates how to estimate $n$
\end{itemize}
\section{Resources}
\subsection{Planetlab}
\subsection{Local Cluster}



\bibliography{notes}
\bibliographystyle{ieeetr}
\end{document}
