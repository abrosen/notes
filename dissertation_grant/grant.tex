\documentclass[12pt,a4paper]{article}
\usepackage{mathptmx}
\usepackage[latin1]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[left=1.00in, right=1.00in, top=1.00in, bottom=1.00in]{geometry}
\author{Andrew Rosen}
\title{Grant Proposal}
\date{}
\begin{document}
\maketitle


%This summary should include a
%very brief summary description of the project including specific aims or goals, a
%statement of the significance or potential impact of the proposed project within the
%field of study and to the world in general, a statement or two about the general
%methods to be employed, and a statement about the expected outcome(s) of the project
%(research question(s) to be answered, new information or data to be uncovered, artistic
%product to be completed, etc.)
\newpage

\begin{abstract}

Distributed Hash Tables (DHTs) are protocols and frameworks used by peer-to-peer (P2P) systems.
They are used as the organizational backbone for many P2P file-sharing systems due to their scalability, fault-tolerance, and load-balancing properties.
These same properties are highly desirable in a distributed computing environment, especially one that wants to use heterogeneous components.

We show that DHTs can be used not only as the framework to build a P2P file-sharing service, but as a P2P distributed computing platform.
We propose creating a P2P distributed computing framework using distributed hash tables, based on our prototype system ChordReduce.
This framework would make it simple and efficient for developers to create their own distributed computing applications.
Unlike Hadoop and similar MapReduce frameworks, our framework can be used both in both the context of a datacenter or as part of a P2P computing platform.  
This opens up new possibilities for building platforms to distributed computing problems.

One advantage our system will have is an autonomous load-balancing mechanism.
Nodes will be able to independently acquire work from other nodes in the network, rather than sitting idle.
More powerful nodes in the network will be able use the mechanism to acquire more work, exploiting the heterogeneity of the network.

We plan on running large scale experiments using both cloud-computing platforms and small, consumer-grade hardware.
This variety of computing device will establish the flexibility of our framework and its ability to use the heterogeniety of the network to its advantage.

By utilizing the load-balancing algorithm, a datacenter could easily leverage additional P2P resources at runtime on an as needed basis.
Our framework will allow MapReduce-like or distributed machine learning platforms to be easily deployed in a greater variety of contexts.
These would include any computationally intensive problems that occur in a wide variety of scientific disciplines, such as biology, chemistry, astronomy and physics.

\end{abstract}

\newpage



%Project Description (no more than three pages, single-spaced): This section should
%outline briefly the past work in the field (i.e. practical, theoretical and empirical work) 
%as a framework for discussing why the work outlined in the proposal is important to
%the field of study and to the world in general. The project description should also
%include the specific aims or goals for the proposed project and provide a clear, detailed
%account of the methodology to be employed and how those methods will lead to the
%expected outcomes. A reference list should be included (if appropriate), but references
%are not subject to the page limitations


\section{Summary}
\subsection*{What are DHTs}
My dissertation focusing on creating a generalized framework for distributed computing based on Distributed Hash Tables (DHTs).
Distributed Hash Tables are a tool which allows a computers (nodes) to self-organize into a decentralized network for storing and retrieving data.
They have been used extensively to build peer-to-peer (P2P) file-sharing applications as well as distributed storage systems.

DHTs are primarily used in P2P applications, but other applications, such as botnets, use DHTs for their decentralization.
Our goal to use DHTs primarily for their intuitive way of organizing a distributed system and to further extend the use of DHTs.
In previous work \cite{chordreduce}, we showed  that a DHT can be to create a distributed computing framework.
We used the same mechanism used in P2P applications that assigns nodes their location in the network to evenly distribute work among members of a DHT.
The most direct application of a DHT distributed computing framework is  a quick and intuitive way to solve embarrassingly parallel problems, such as:
\begin{itemize}
	\item Brute force cryptography.
	\item Genetic algorithms.
	\item Markov chain Monte Carlo methods.
	\item Random forests.
	\item Any problem that could be phrased as a MapReduce problem.
	
\end{itemize}
Unlike the current distributed applications that utilize DHTs, we want to create a complete framework that can be used to build decentralized applications.
We have found no existing projects that provide a means of building your own DHT or DHT based applications. %without a given DHT in mind at least







\subsection*{Completed Work}

There are many different types of DHTs, but all DHTs share very important qualities.
They are \textit{scalable}, which means that each additional node in the network minimally impacts the cost of keeping the network organized.
DHTs are also highly \textit{fault-tolerant}.
Unlike many other systems, DHTs assume that nodes will be continuously entering and leaving the network.
Because of the way DHTs are organized, they can handle large scale failures, such as power outage affecting an entire city.
The last quality of DHTs is that they are  \textit{load-balancing}. 
This means the data stored in a DHTs is evenly distributed among nodes in the network.


All of these qualities are highly desirable in a distributed computing, so it was a natural step for us to build distributed computing framework based on DHTs.
We have shown in previous work that DHTs can be used to distribute work among nodes the same way data is distributed \cite{chordreduce}.
MOAR SENTANCE ChordReduce was a proof of concept

DGVH


We built UrDHT \cite{urdht}, a generalized framework for building distributed hash tables, using the concepts we discovered creating DGVH.
The novelty of UrDHT is that it makes it easy for scientists in any field to construct a DHT based application.
For computer scientists, UrDHT provides a means of rigorously comparing different architectures.

\subsection*{Remaining Work}

Distributed Computing using URDHT
This is different than ChordReduce because we didn't do large scale tests.


Autonomous load balancing
Anomaly during experimentation 
Reverse a security flaw for good of network


\subsubsection*{Why is this exciting }
Our framework would not only be of interest to computer scientists.
The ``plug-and-play'' nature we envision would make it easy for experts in other fields to use our framework to solve computationally intensive problems.
Some example problems are Monte-Carlo Methods (of interest to mathematicians and economists), 


\subsection*{Methodology}

The method to validate our hypotheses and theorical models is through large-scale  real world experiments using variety of devices across the world.
A cluster won't do
We need multiple clusters essentially,
An effective approach is purchasing cloud computing resources from Amazon and Google.



Heterogeneous networks
We need lower powered devices, which are individually inexpensive and easily deployable.

I need hardware of different types and locations.





We have successfully developed our software to achieve this, but need funding to do real world network tests. 
These tests would allow us to experimentally show that our software conclusively works on a large scale and on heterogeneous networks.




This project would allow both organizations with large amounts of computing power and average developers with  fewer resources to spend less time setting up and configuring their hardware to work together.  
The goal is for our software to make distributed computing more of matter of ``plug-and-play,'' allowing researchers to spend less time setting up and maintaining  computing platforms.


\bibliography{mine,dht}
\bibliographystyle{plain}
\newpage



\section{Vita goes here}

\section*{Education}          
    
{\bf Ph.D.} in Computer Science, Georgia State University. May 2016 (Expected)\\
{\bf M.S.} in Computer Science, Georgia State University. May 2014, 3.89 GPA \\
{\bf B.S.} in Computer Science, Georgia Institute of Technology. May 2010, 3.00 GPA\\
{\bf Minor} in Music, Georgia Institute of Technology. May 2010\\
	
\section*{Appointments}
{\bf 2CI Astroinformatics Fellow,} Georgia State University, Aug 2012 - Present\\
{\bf Graduate Research Assistant,} Georgia State University, Aug 2011 - Present\\
{\bf Graduate Lab Assistant,} Georgia State University, May 2011 - 2013

\section*{Publications}
\begin{enumerate}
	\item Andrew Rosen, Brendan Benshoof, Robert W. Harrison, Anu G. Bourgeois
	``MapReduce on a Chord Distributed Hash Table''
	Presentation ICA CON 2014, Poster at IPDPS 2014 PhD Forum
	\item Brendan Benshoof, Andrew Rosen, Anu G. Bourgeois, Robert W. Harrison
	``VHASH: Spatial DHT based on Voronoi Tessellation''
	ICA CON 2014
	\item  Erin-Elizabeth A. Durham, Andrew Rosen, Robert W. Harrison
	``A Model Architecture for Big Data applications using Relational Databases''
	2014 IEEE BigData - C4BD2014 - Workshop on Complexity for Big Data
	\item Chinua Umoja, J.T. Torrance, Erin-Elizabeth A. Durham, Andrew Rosen, Dr. Robert Harrison
	``A Novel Approach to Determine Docking Locations Using Fuzzy Logic and Shape Determination''
	2014 IEEE BigData - Poster and Short Paper
	\item  Erin-Elizabeth A. Durham, Andrew Rosen, Robert W. Harrison
	``Optimization of Relational Database Usage Involving Big Data''
	IEEE SSCI 2014 - CIDM 2014 - The IEEE Symposium Series on Computational Intelligence and Data Mining
	\item Brendan Benshoof, Andrew Rosen, Anu G. Bourgeois, Robert W. Harrison
	``A Distributed Greedy Heuristic for Computing Voronoi Tessellations With Applications Towards Peer-to-Peer Network''
	IEEE IPDPS 2015 - Workshop on Dependable Parallel, Distributed and Network-Centric Systems
	\item Chaoyang Li, Andrew Rosen,  and Anu G. Bourgeois
	``A Novel Approach to Efficiently Detect 3D Full-View Coverage for Camera Sensor Networks''
	Submitted to IPDPS 2016 
	\item Andrew Rosen, Brendan Benshoof, Robert W. Harrison, Anu G. Bourgeois
	``UrDHT: A Unified Model for Distributed Hash Tables''
	Submitted to IPDPS 2016
	\item Andrew Rosen, Brendan Benshoof, Robert W. Harrison, Anu G. Bourgeois
	``The Sybil Attack on Peer-to-Peer Networks From the Attacker?s Perspective''
	In preparation
	
\end{enumerate}


\section*{Service}
{\bf Vice Chair,} Georgia State University Chapter of the Association for Computing Machinery (ACM),  May 2012 - Present\\
{\bf Treasurer,} Georgia State University Chapter of the Association for Computing Machinery (ACM), May 2012 - May 2014\\
{\bf New Graduate Student Orientation Panelist}, Georgia State University, 2014-2015\\
{\bf Department Representative,} Georgia State University Arts and Sciences Tech Fee Committee, 2013 - 2015\\

\section*{External Funding}
\textbf{TCPP Travel Grant} for 28th IEEE International Parallel \& Distributed Processing Symposium

\section*{Honors and Awards}
\textbf{Outstanding Graduate Student Teaching Award}, Department of Computer Science, Georgia State University, 2014

\section*{Employment}
{\bf Instructor}, Georgia State University, Spring 2012 (CSc 3410), Spring 2013,\\
{\bf Developer}, Georgia Tech Sonification Lab, Atlanta, GA May-Dec 2010\\
{\bf Undergraduate Researcher}, Georgia Tech Sonification Lab, Atlanta, GA Fall 2007 -Dec 2009


\newpage 
\section{Budget and Justification}

\begin{table}[h!]
\centering{
	\begin{tabular}{|c|r|}
		
		\hline Item & Amount requested \\
		\hline Cloud Computing Server Time & \$1300 \\ 
		\hline 12x Raspberry PI 2 & \$480 \\
		\hline Raspberry PI peripherals & \$250\\
		%	\hline Volunteer Recruitment &  \$150 \\
		%	\hline Travel & \$1000 \\
		\hline \hline Total Amount requested  & \$2000 \\
		\hline
	\end{tabular} 
}


\end{table}

The base cost for a cloud computing machine ranges between \$0.008 and \$0.075 per machine per hour.
We need at least 100 machines for at least 100 hours of experimentation at the bare minimum.
Buying cloud computing time from both Google and Amazon would allow us to do realistic networks.

However, we still need more machines in order to test heterogeneity in the network.
Raspberry PIs allow us to test the lower end of computing power.
Raspberry PIs are credit-card sized computers which are powerful enough to test our software, but much weaker in computational power compared to what we can purchase from Amazon.
They would allow us to our autonomous load-balancing and see how well our algorithms distribute work between stronger and weaker nodes.
Furthermore, we could attach some to different network interfaces, making some wired and some wireless, which would allow us to further vary our network topology.
%
%Recruiting volunteers would add greater amounts of realism to our experiment.
%Pending IRB approval, we would use these funds to advertise on online technical sites and ask users to run our program.
%No information would be gathered from the users other than their IP address and a relative measure of their computational power


\end{document}